[
  {
    "objectID": "introduction.html",
    "href": "introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "Respiratory illnesses are among the most common chronic health issues, negatively impacting the daily lives of millions of people worldwide and placing substantial burdens on healthcare systems. In the United States alone, more than 15.7 million people suffer from Chronic Obstructive Pulmonary Disease (COPD), which, along with asthma and other chronic lower respiratory diseases, remains a leading cause of death. Although there is currently no cure for COPD or asthma, proper medication and preventative measures can help control the manifestation of severe symptoms and slow disease progression. Individuals with preexisting respiratory conditions, such as asthma and COPD, also face increased risks of severe illness from other diseases, including COVID-19.\nAn essential component of preventative care for respiratory illnesses lies in understanding the integral role environmental and personal factors play in the manifestation of symptoms. Behavioral factors, such as smoking tobacco, are well-documented contributors to COPD risk. Local environmental factors, including proximity to high-density development, forests, grasslands, or long-term exposure to pollens, can also exacerbate respiratory conditions, such as asthma. Additionally, sociodemographic factors—such as income, education, and healthcare access—are crucial determinants of individual health outcomes.\nIn geospatial health studies, predictive models are widely used to analyze disease outbreaks by integrating geographic variables, sociodemographic factors, and remote sensing data as proxies for local environmental conditions. These models, traditionally built on regression methods, have evolved to include machine learning techniques that can account for nonlinearity and complex interactions among variables.\nThe recent COVID-19 pandemic has underscored the critical need for risk identification, as individuals with preexisting respiratory conditions often experience more severe symptoms and higher mortality rates. Although COPD is not transmissible, these outcomes highlight the vulnerability of individuals with chronic illnesses during widespread infectious disease outbreaks. Identifying high-risk groups at a neighborhood scale is essential for targeted interventions, which can help reduce exposure and mitigate adverse outcomes during future pandemics. However, several challenges complicate this type of analysis. These challenges include the cost-prohibitive nature of highly localized data collection, difficulties in accessing confidential medical records, the complexity of interactions among determinants, and variations in human mobility and exposure to environmental conditions.\nTo address these challenges, this study investigates the determinants of chronic respiratory diseases at the neighborhood scale in Pennsylvani in 2022. By leveraging census tract-level data, tobacco retailer density, health behavior surveys, seasonal land cover classifications, and satellite-derived vegetation indices, we aim to identify significant predictors of COPD prevalence. Using multiple analytic techniques—such as multiple linear regression, multilayer perceptron, support vector regression, and random forest regression—we seek to provide a nuanced understanding of spatial disparities in respiratory health and enhance prediction accuracy across Pennsylvania.\n\n\nIn recent years, remote sensing data have become increasingly valuable in monitoring, spatial predictive modeling, and risk assessment related to human health. These data enable researchers to identify social and environmental determinants of health while offering consistent spatial and temporal coverage. Simultaneously, machine learning techniques have gained traction in public health research for their ability to improve disease prediction and early detection, optimize healthcare resource allocation, and facilitate the development of personalized treatment plans.\nThis project builds upon several existing studies that integrate remote sensing data and machine learning techniques in health research. For example, Alvarez-Mendoza et al. (2020) examined the relationship between remote sensing data, air quality variables, and hospital discharge rates of patients with chronic respiratory diseases (CRDs) in Quito, Ecuador. Their study compared three machine learning methods—support vector regression, random forest regression, and multilayer perceptron—to predict CRD rates, incorporating environmental variables, air pollution measurements, and meteorological data. This research provided insights into the most significant spatial predictors and highlighted the spatial distribution of CRD prevalence within Quito.\nSimilarly, Rehman and Counts (2021) explored neighborhood-level chronic respiratory disease prevalence by using search query data alongside land cover and census data as proxies for the local built environment. Their study employed a random forest regression model to predict disease prevalence, emphasizing the role of environmental and sociodemographic factors. Leas et al. (2019) further investigated place-based inequities in smoking prevalence within the largest cities in the United States. Using linear mixed models, they characterized smoking prevalence inequities within and between cities, assessed tract-level smoking prevalence as a function of sociodemographic characteristics and tobacco retailer density, and evaluated the association between smoking prevalence and tract-level asthma, COPD, and coronary heart disease (CHD) prevalence. Their findings underscored the connections between smoking behavior, tobacco retailer density, and respiratory health outcomes.\nThese studies collectively demonstrate the utility of integrating remote sensing data, sociodemographic information, and machine learning techniques in analyzing spatial health disparities. By drawing on these approaches, our work aims to advance the understanding of neighborhood-scale respiratory health outcomes and refine predictive modeling methods.\n\n\n\nTo establish our machine learning models, this project utilizes multiple data sources, including the 500 Cities & PLACES Data Portal, the American Community Survey (ACS), the National Land Cover Database (NLCD), and satellite imagery from Landsat 8 processed through Google Earth Engine.\n\n\nThe most geographically granular data for identifying COPD risk in the U.S. comes from the Centers for Disease Control and Prevention (CDC). In 2015, the CDC, in partnership with the Robert Wood Johnson Foundation, began collecting small-area estimates for 27 chronic disease measures, including respiratory illnesses, at the census-tract level across 500 major U.S. cities. Census tracts represent the smallest geographic units in the U.S. for which comprehensive population, socioeconomic, and demographic data are available. This dataset includes estimates for 40 measures, spanning various categories:\n\nHealth outcomes (12 measures)\nPreventive services use (7 measures)\nChronic disease-related health risk behaviors (4 measures)\nDisabilities (7 measures)\nHealth status (3 measures)\nHealth-related social needs (7 measures)\n\nThe model-based estimates are derived from several sources, including the Behavioral Risk Factor Surveillance System (BRFSS) 2021 or 2022 data, Census Bureau 2020 population data, and ACS 2018–2022 estimates. For this study, we specifically focus on COPD prevalence rates and the four chronic disease-related health risk measures.\n\n\n\nRemote sensing data serve as effective proxies for understanding local environmental conditions. They are easily retrievable, accurate, open-source, and enable rapid analysis, making them invaluable for health and environmental studies.\nOne of the most renowned satellite remote sensing missions is the Landsat program, which began in 1972. The latest satellite in this program, Landsat 8, is equipped with the Operational Land Imager (OLI) and the Thermal Infrared Sensor (TIRS). Landsat 8 provides a comprehensive spatial and temporal perspective of the Earth, supporting a range of applications and enabling the retrieval of variables such as vegetation, land use, aerosol particles, and meteorological data.\nFor this study, we utilize Landsat 8 Level 2 images from Spring 2022 to Spring 2023 over the study area. The primary advantage of these images is that they include geometrical, radiometric, and atmospheric corrections using the Landsat 8 Surface Reflectance Code. Specifically, we employ the surface reflectance OLI bands to generate brightness temperature (BT) and various pre-processed vegetation indices, such as the Normalized Difference Vegetation Index (NDVI), the Soil-Adjusted Vegetation Index (SAVI), and the Enhanced Vegetation Index (EVI).\nAdditionally, we incorporate the latest land cover data as proxies for local environmental characteristics. The U.S. Geological Survey (USGS), in collaboration with several federal agencies, publishes the National Land Cover Database (NLCD). This dataset offers nationwide land cover classifications derived from 30-meter resolution satellite imagery. It includes eight main land cover classes and 20 sub-classes, ranging from high-density development areas to grasslands, open water, and forests. The NLCD is publicly available and provides a critical resource for analyzing the interplay between land cover and health outcomes.\n\n\n\nThis study incorporates two additional data sources: the American Community Survey (ACS) five-year estimates (2022) and tobacco retailer data from the Pennsylvania Open Data Portal.\nThe ACS dataset, which is publicly available and accessible via the Census API, provides comprehensive demographic and socioeconomic information at various geographic levels. For this analysis, we focus on attributes such as disability status, age, race, and ethnicity. These variables are particularly relevant as they help identify populations vulnerable to Chronic Obstructive Pulmonary Disease (COPD) and other chronic respiratory conditions.\nThe tobacco retailer data, sourced from the Pennsylvania Open Data Portal, offers insights into the density and distribution of tobacco retailers across the state. We include this dataset because tobacco retailer density and smoking prevalence are well-established proxies for environmental exposure to health risks. This information enables a deeper understanding of local environmental factors contributing to respiratory illnesses.\nNote: This section serves to introduce the data sources. Detailed discussions on attribute definitions, data extraction, transformation, and processing will be provided in the methodology section. \n\n\n\n\nBelow is the workflow diagram for this project, along with a brief description of each step:\n\nFirst, Landsat data is processed by applying scale factors and cloud masking. A median composite image for the 2022 study period is then calculated, categorized by season, and clipped to the area of interest (Pennsylvania). After calculating the relevant indices for each season, zonal statistics are applied, and pixel values are aggregated to the census tract level.\nNext, the National Land Cover Database (NLCD) data is geo-referenced using the boundaries of the census tracts. The proportion of each land cover sub-class within each census tract is calculated. To capture variations in the distribution of land cover, additional features are introduced, such as the proportion of highly developed land pixels neighboring other highly developed land pixels and the proportion of forest pixels neighboring other forest pixels. These features account for spatial patterns that could influence health outcomes, such as pollution from developed areas and exposure to pollen from forests, both of which may impact individuals with asthma and COPD.\nThe tobacco retailer data is then aggregated to the census tract level, and the density of tobacco retailers is computed. To account for environmental exposures from neighboring tracts, the density from neighboring census tracts is averaged using queen contiguity, acknowledging that individuals often travel outside their census tracts for work or recreational purposes.\nCensus data from the American Community Survey (ACS) is also incorporated, with a focus on race, disability, and age indicators. For each census tract, the proportions of the population that are minorities, elderly, or disabled are calculated. Additionally, the CDC’s chronic disease risk data, which includes health risk behavior indicators, is joined to the census tract dataset. To estimate the seasonal prevalence of COPD, existing literature is used, as the original dataset provides only annual rates.\nOnce all data sources are merged into a single dataframe for Pennsylvania census tracts, null values are replaced with zeros, and census tracts with missing data are removed. To reduce multicollinearity, Bayesian Information Criterion (BIC) is used to select predictor variables. Finally, several machine learning models, including random forests, multi-layer perceptrons, and support vector regression, are run. The models are evaluated using R² and RMSE (Root Mean Square Error), and the most important predictors for each season are identified, revealing the key factors influencing COPD prevalence.\nNote: This section serves to introduce the workflow. Details on the execution of each step, particularly using Python, will be provided in the methodology and analysis section.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "introduction.html#related-work",
    "href": "introduction.html#related-work",
    "title": "Introduction",
    "section": "",
    "text": "In recent years, remote sensing data have become increasingly valuable in monitoring, spatial predictive modeling, and risk assessment related to human health. These data enable researchers to identify social and environmental determinants of health while offering consistent spatial and temporal coverage. Simultaneously, machine learning techniques have gained traction in public health research for their ability to improve disease prediction and early detection, optimize healthcare resource allocation, and facilitate the development of personalized treatment plans.\nThis project builds upon several existing studies that integrate remote sensing data and machine learning techniques in health research. For example, Alvarez-Mendoza et al. (2020) examined the relationship between remote sensing data, air quality variables, and hospital discharge rates of patients with chronic respiratory diseases (CRDs) in Quito, Ecuador. Their study compared three machine learning methods—support vector regression, random forest regression, and multilayer perceptron—to predict CRD rates, incorporating environmental variables, air pollution measurements, and meteorological data. This research provided insights into the most significant spatial predictors and highlighted the spatial distribution of CRD prevalence within Quito.\nSimilarly, Rehman and Counts (2021) explored neighborhood-level chronic respiratory disease prevalence by using search query data alongside land cover and census data as proxies for the local built environment. Their study employed a random forest regression model to predict disease prevalence, emphasizing the role of environmental and sociodemographic factors. Leas et al. (2019) further investigated place-based inequities in smoking prevalence within the largest cities in the United States. Using linear mixed models, they characterized smoking prevalence inequities within and between cities, assessed tract-level smoking prevalence as a function of sociodemographic characteristics and tobacco retailer density, and evaluated the association between smoking prevalence and tract-level asthma, COPD, and coronary heart disease (CHD) prevalence. Their findings underscored the connections between smoking behavior, tobacco retailer density, and respiratory health outcomes.\nThese studies collectively demonstrate the utility of integrating remote sensing data, sociodemographic information, and machine learning techniques in analyzing spatial health disparities. By drawing on these approaches, our work aims to advance the understanding of neighborhood-scale respiratory health outcomes and refine predictive modeling methods.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "introduction.html#data-collection",
    "href": "introduction.html#data-collection",
    "title": "Introduction",
    "section": "",
    "text": "To establish our machine learning models, this project utilizes multiple data sources, including the 500 Cities & PLACES Data Portal, the American Community Survey (ACS), the National Land Cover Database (NLCD), and satellite imagery from Landsat 8 processed through Google Earth Engine.\n\n\nThe most geographically granular data for identifying COPD risk in the U.S. comes from the Centers for Disease Control and Prevention (CDC). In 2015, the CDC, in partnership with the Robert Wood Johnson Foundation, began collecting small-area estimates for 27 chronic disease measures, including respiratory illnesses, at the census-tract level across 500 major U.S. cities. Census tracts represent the smallest geographic units in the U.S. for which comprehensive population, socioeconomic, and demographic data are available. This dataset includes estimates for 40 measures, spanning various categories:\n\nHealth outcomes (12 measures)\nPreventive services use (7 measures)\nChronic disease-related health risk behaviors (4 measures)\nDisabilities (7 measures)\nHealth status (3 measures)\nHealth-related social needs (7 measures)\n\nThe model-based estimates are derived from several sources, including the Behavioral Risk Factor Surveillance System (BRFSS) 2021 or 2022 data, Census Bureau 2020 population data, and ACS 2018–2022 estimates. For this study, we specifically focus on COPD prevalence rates and the four chronic disease-related health risk measures.\n\n\n\nRemote sensing data serve as effective proxies for understanding local environmental conditions. They are easily retrievable, accurate, open-source, and enable rapid analysis, making them invaluable for health and environmental studies.\nOne of the most renowned satellite remote sensing missions is the Landsat program, which began in 1972. The latest satellite in this program, Landsat 8, is equipped with the Operational Land Imager (OLI) and the Thermal Infrared Sensor (TIRS). Landsat 8 provides a comprehensive spatial and temporal perspective of the Earth, supporting a range of applications and enabling the retrieval of variables such as vegetation, land use, aerosol particles, and meteorological data.\nFor this study, we utilize Landsat 8 Level 2 images from Spring 2022 to Spring 2023 over the study area. The primary advantage of these images is that they include geometrical, radiometric, and atmospheric corrections using the Landsat 8 Surface Reflectance Code. Specifically, we employ the surface reflectance OLI bands to generate brightness temperature (BT) and various pre-processed vegetation indices, such as the Normalized Difference Vegetation Index (NDVI), the Soil-Adjusted Vegetation Index (SAVI), and the Enhanced Vegetation Index (EVI).\nAdditionally, we incorporate the latest land cover data as proxies for local environmental characteristics. The U.S. Geological Survey (USGS), in collaboration with several federal agencies, publishes the National Land Cover Database (NLCD). This dataset offers nationwide land cover classifications derived from 30-meter resolution satellite imagery. It includes eight main land cover classes and 20 sub-classes, ranging from high-density development areas to grasslands, open water, and forests. The NLCD is publicly available and provides a critical resource for analyzing the interplay between land cover and health outcomes.\n\n\n\nThis study incorporates two additional data sources: the American Community Survey (ACS) five-year estimates (2022) and tobacco retailer data from the Pennsylvania Open Data Portal.\nThe ACS dataset, which is publicly available and accessible via the Census API, provides comprehensive demographic and socioeconomic information at various geographic levels. For this analysis, we focus on attributes such as disability status, age, race, and ethnicity. These variables are particularly relevant as they help identify populations vulnerable to Chronic Obstructive Pulmonary Disease (COPD) and other chronic respiratory conditions.\nThe tobacco retailer data, sourced from the Pennsylvania Open Data Portal, offers insights into the density and distribution of tobacco retailers across the state. We include this dataset because tobacco retailer density and smoking prevalence are well-established proxies for environmental exposure to health risks. This information enables a deeper understanding of local environmental factors contributing to respiratory illnesses.\nNote: This section serves to introduce the data sources. Detailed discussions on attribute definitions, data extraction, transformation, and processing will be provided in the methodology section.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "introduction.html#study-workflow",
    "href": "introduction.html#study-workflow",
    "title": "Introduction",
    "section": "",
    "text": "Below is the workflow diagram for this project, along with a brief description of each step:\n\nFirst, Landsat data is processed by applying scale factors and cloud masking. A median composite image for the 2022 study period is then calculated, categorized by season, and clipped to the area of interest (Pennsylvania). After calculating the relevant indices for each season, zonal statistics are applied, and pixel values are aggregated to the census tract level.\nNext, the National Land Cover Database (NLCD) data is geo-referenced using the boundaries of the census tracts. The proportion of each land cover sub-class within each census tract is calculated. To capture variations in the distribution of land cover, additional features are introduced, such as the proportion of highly developed land pixels neighboring other highly developed land pixels and the proportion of forest pixels neighboring other forest pixels. These features account for spatial patterns that could influence health outcomes, such as pollution from developed areas and exposure to pollen from forests, both of which may impact individuals with asthma and COPD.\nThe tobacco retailer data is then aggregated to the census tract level, and the density of tobacco retailers is computed. To account for environmental exposures from neighboring tracts, the density from neighboring census tracts is averaged using queen contiguity, acknowledging that individuals often travel outside their census tracts for work or recreational purposes.\nCensus data from the American Community Survey (ACS) is also incorporated, with a focus on race, disability, and age indicators. For each census tract, the proportions of the population that are minorities, elderly, or disabled are calculated. Additionally, the CDC’s chronic disease risk data, which includes health risk behavior indicators, is joined to the census tract dataset. To estimate the seasonal prevalence of COPD, existing literature is used, as the original dataset provides only annual rates.\nOnce all data sources are merged into a single dataframe for Pennsylvania census tracts, null values are replaced with zeros, and census tracts with missing data are removed. To reduce multicollinearity, Bayesian Information Criterion (BIC) is used to select predictor variables. Finally, several machine learning models, including random forests, multi-layer perceptrons, and support vector regression, are run. The models are evaluated using R² and RMSE (Root Mean Square Error), and the most important predictors for each season are identified, revealing the key factors influencing COPD prevalence.\nNote: This section serves to introduce the workflow. Details on the execution of each step, particularly using Python, will be provided in the methodology and analysis section.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "code/Machine_Learning.html",
    "href": "code/Machine_Learning.html",
    "title": "Running Machine Learning Model",
    "section": "",
    "text": "from google.colab import drive\ndrive.mount('/content/drive')\n\nMounted at /content/drive\nimport numpy as np\nimport pandas as pd\nimport geopandas as gpd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVR\nfrom sklearn.metrics import mean_squared_error, r2_score,  mean_absolute_error\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nimport tensorflow as tf\nfrom scipy.stats import norm\nimport warnings\nwarnings.filterwarnings('ignore')\nPA_Final = pd.read_csv('/content/drive/MyDrive/PA_Final.csv')\nPA_Final.columns\n\nIndex(['CountyName', 'LocationName', 'Asthma', 'TotalPopulation',\n       'TotalPop18plus', 'COP', 'GEOID', 'Smoking', 'Drinking', 'Short_Sleep',\n       'Physical_Activity', 'neighbor_avg_density', 'minority', 'aging',\n       'disability', 'forest_total', 'wetland_total', 'Hdensity_total',\n       'Ldensity_total', 'grasses_total', 'water_total', 'forest_pct',\n       'Hdensity_pct', 'lst_winter', 'ndvi_fall', 'evi_spring', 'evi_fall',\n       'savi_fall', 'savi_winter', 'savi_summer', 'savi_spring', 'ndvi_winter',\n       'lst_summer', 'evi_summer', 'lst_spring', 'lst_fall', 'ndvi_spring',\n       'ndvi_summer', 'evi_winter'],\n      dtype='object')\nThe COPD exacerbation rate was higher in winter (0.13 exacerbations/person-month) than in spring, summer, and fall (0.11, 0.079, and 0.10 exacerbations/person-month, respectively) (P &lt; 0.001). Summer had the highest proportion of severe exacerbations (40.5%) compared with spring, fall, and winter (32.6%, 34.7%, and 33.1%, respectively) (P = 0.004).\ndef calculate_seasonal_exacerbations_df(df, total_column, seasonal_rates):\n    \"\"\"\n    Apply the seasonal exacerbation calculation to a DataFrame column containing total exacerbations.\n\n    :param df: The DataFrame containing the total exacerbations column\n    :param total_column: The name of the column containing total exacerbations for each tract\n    :param seasonal_rates: Dictionary with seasonal rates for each season (winter, spring, summer, fall)\n\n    :return: The DataFrame with new columns for seasonal exacerbations\n    \"\"\"\n    # Normalize the seasonal rates\n    total_rate = sum(seasonal_rates.values())\n    normalized_rates = {season: rate / total_rate for season, rate in seasonal_rates.items()}\n\n    # Calculate seasonal exacerbations for each row in the DataFrame\n    for season, rate in normalized_rates.items():\n        df[f'{season}_count'] = df[total_column] * rate\n        df[f'{season}_rate'] = df[f'{season}_count'] / df['TotalPopulation'] * 100\n\n    return df\nseasonal_rates = {\n    'winter': 0.39,  # Winter rate\n    'spring': 0.33,  # Spring rate\n    'summer': 0.237, # Summer rate\n    'fall': 0.30     # Fall rate\n}\n\nPA_Final[\"COP_People\"] = PA_Final[\"COP\"] / 100 * PA_Final[\"TotalPopulation\"]\nPA_Final = calculate_seasonal_exacerbations_df(PA_Final, 'COP_People', seasonal_rates)\nPA_Final.columns\n\nIndex(['CountyName', 'LocationName', 'Asthma', 'TotalPopulation',\n       'TotalPop18plus', 'COP', 'GEOID', 'Smoking', 'Drinking', 'Short_Sleep',\n       'Physical_Activity', 'neighbor_avg_density', 'minority', 'aging',\n       'disability', 'forest_total', 'wetland_total', 'Hdensity_total',\n       'Ldensity_total', 'grasses_total', 'water_total', 'forest_pct',\n       'Hdensity_pct', 'lst_winter', 'ndvi_fall', 'evi_spring', 'evi_fall',\n       'savi_fall', 'savi_winter', 'savi_summer', 'savi_spring', 'ndvi_winter',\n       'lst_summer', 'evi_summer', 'lst_spring', 'lst_fall', 'ndvi_spring',\n       'ndvi_summer', 'evi_winter', 'COP_People', 'winter_count',\n       'winter_rate', 'spring_count', 'spring_rate', 'summer_count',\n       'summer_rate', 'fall_count', 'fall_rate'],\n      dtype='object')\n# spring dataframe\nPA_Spring = PA_Final[['CountyName', 'Asthma', 'TotalPopulation', 'COP', 'GEOID', 'Smoking',\n                      'Drinking', 'Short_Sleep', 'Physical_Activity', 'neighbor_avg_density',\n                      'minority', 'aging', 'disability', 'forest_total', 'wetland_total', 'Hdensity_total',\n                      'Ldensity_total', 'grasses_total', 'water_total', 'forest_pct',  'Hdensity_pct',\n                      'evi_spring','lst_spring', 'ndvi_spring', 'savi_spring', 'spring_count', \"spring_rate\"]]\n# summer dataframe\nPA_Summer = PA_Final[['CountyName', 'Asthma', 'TotalPopulation', 'COP', 'GEOID', 'Smoking',\n                      'Drinking', 'Short_Sleep', 'Physical_Activity', 'neighbor_avg_density',\n                      'minority', 'aging', 'disability', 'forest_total', 'wetland_total', 'Hdensity_total',\n                      'Ldensity_total', 'grasses_total', 'water_total', 'forest_pct',  'Hdensity_pct',\n                      'evi_summer','lst_summer', 'ndvi_summer', 'savi_summer', 'summer_count', \"summer_rate\"]]\n# fall dataframe\nPA_Fall = PA_Final[['CountyName', 'Asthma', 'TotalPopulation', 'COP', 'GEOID', 'Smoking',\n                      'Drinking', 'Short_Sleep', 'Physical_Activity', 'neighbor_avg_density',\n                      'minority', 'aging', 'disability', 'forest_total', 'wetland_total', 'Hdensity_total',\n                      'Ldensity_total', 'grasses_total', 'water_total', 'forest_pct',  'Hdensity_pct',\n                      'evi_fall','lst_fall', 'ndvi_fall', 'savi_fall', 'fall_count', \"fall_rate\"]]\n# winter dataframe\nPA_Winter = PA_Final[['CountyName', 'Asthma', 'TotalPopulation', 'COP', 'GEOID', 'Smoking',\n                      'Drinking', 'Short_Sleep', 'Physical_Activity', 'neighbor_avg_density',\n                      'minority', 'aging', 'disability', 'forest_total', 'wetland_total', 'Hdensity_total',\n                      'Ldensity_total', 'grasses_total', 'water_total', 'forest_pct',  'Hdensity_pct',\n                      'evi_winter','lst_winter', 'ndvi_winter', 'savi_winter', 'winter_count', \"winter_rate\"]]"
  },
  {
    "objectID": "code/Machine_Learning.html#spring-model",
    "href": "code/Machine_Learning.html#spring-model",
    "title": "Running Machine Learning Model",
    "section": "Spring Model",
    "text": "Spring Model\n\n# using all variables\nstratify_col = PA_Spring['CountyName']\nX_Spring_All = PA_Spring[['Smoking', 'Drinking', 'Short_Sleep', 'Physical_Activity', 'neighbor_avg_density',\n                    'minority', 'aging', 'disability', 'forest_total', 'wetland_total', 'Hdensity_total',\n                    'Ldensity_total', 'grasses_total', 'water_total', 'forest_pct',  'Hdensity_pct',\n                    'evi_spring','lst_spring', 'ndvi_spring', 'savi_spring']]\ny_Spring = PA_Spring[['spring_count']]\nX_train_scaled, X_test_scaled, y_train, y_test_sp, scaler = split_and_scale_data(X_Spring_All, y_Spring, stratify_col)\nspring_result = train_and_evaluate_rf(X_train_scaled, X_test_scaled, y_train, y_test_sp, X_Spring_All.columns)\n\n\n# using all variables after dimensionality reduction\nX_Spring_BIC = PA_Spring[['Smoking', 'Drinking', 'Short_Sleep', 'Physical_Activity', 'minority', 'aging', 'disability', 'forest_total', 'Ldensity_total', 'grasses_total', 'forest_pct', 'lst_spring']]\nX_train_scaled, X_test_scaled, y_train, y_test_sp, scaler = split_and_scale_data(X_Spring_BIC, y_Spring, stratify_col)\nspring_result_BIC = train_and_evaluate_rf(X_train_scaled, X_test_scaled, y_train, y_test_sp, X_Spring_BIC.columns)\n\n\n# using all environmental predictors\nX_Spring_Env = PA_Spring[['forest_total', 'wetland_total', 'Hdensity_total',\n                    'Ldensity_total', 'grasses_total', 'water_total', 'forest_pct',  'Hdensity_pct',\n                    'evi_spring','lst_spring', 'ndvi_spring', 'savi_spring']]\nX_train_scaled, X_test_scaled, y_train, y_test_sp, scaler = split_and_scale_data(X_Spring_Env, y_Spring, stratify_col)\nspring_result_env = train_and_evaluate_rf(X_train_scaled, X_test_scaled, y_train, y_test_sp, X_Spring_Env.columns)\n\n\n# using all socio-behavioral predictors\nX_Spring_Sol = PA_Spring[['Smoking', 'Drinking', 'Short_Sleep', 'Physical_Activity', 'neighbor_avg_density',\n                      'minority', 'aging', 'disability']]\nX_train_scaled, X_test_scaled, y_train, y_test_sp, scaler = split_and_scale_data(X_Spring_Sol, y_Spring, stratify_col)\nspring_result_sol = train_and_evaluate_rf(X_train_scaled, X_test_scaled, y_train, y_test_sp, X_Spring_Sol.columns)"
  },
  {
    "objectID": "code/Machine_Learning.html#summer-model",
    "href": "code/Machine_Learning.html#summer-model",
    "title": "Running Machine Learning Model",
    "section": "Summer Model",
    "text": "Summer Model\n\n# using all variables\nstratify_col = PA_Summer['CountyName']\nX_Summer_All = PA_Summer[['Smoking', 'Drinking', 'Short_Sleep', 'Physical_Activity', 'neighbor_avg_density',\n                    'minority', 'aging', 'disability', 'forest_total', 'wetland_total', 'Hdensity_total',\n                    'Ldensity_total', 'grasses_total', 'water_total', 'forest_pct',  'Hdensity_pct',\n                    'evi_summer','lst_summer', 'ndvi_summer', 'savi_summer']]\ny_Summer = PA_Summer[['summer_count']]\nX_train_scaled, X_test_scaled, y_train, y_test_su, scaler = split_and_scale_data(X_Summer_All, y_Summer, stratify_col)\nsummer_result = train_and_evaluate_rf(X_train_scaled, X_test_scaled, y_train, y_test_su, X_Summer_All.columns)\n\n\n# using all variables after dimensionality reduction\nX_Summer_BIC = PA_Summer[['Smoking', 'Drinking', 'Short_Sleep', 'Physical_Activity', 'minority', 'aging', 'disability', 'forest_total', 'Ldensity_total', 'grasses_total', 'forest_pct', 'lst_summer']]\nX_train_scaled, X_test_scaled, y_train, y_test_su, scaler = split_and_scale_data(X_Summer_BIC, y_Summer, stratify_col)\nsummer_result_BIC = train_and_evaluate_rf(X_train_scaled, X_test_scaled, y_train, y_test_su, X_Summer_BIC.columns)\n\n\n# using only environment variables\nX_Summer_Env = PA_Summer[['forest_total', 'wetland_total', 'Hdensity_total',\n                    'Ldensity_total', 'grasses_total', 'water_total', 'forest_pct',  'Hdensity_pct',\n                    'evi_summer','lst_summer', 'ndvi_summer', 'savi_summer']]\nX_train_scaled, X_test_scaled, y_train, y_test_su, scaler = split_and_scale_data(X_Summer_Env, y_Summer, stratify_col)\nsummer_result_env = train_and_evaluate_rf(X_train_scaled, X_test_scaled, y_train, y_test_su, X_Summer_Env.columns)\n\n\n# using only social behavorial variables\nX_Summer_Sol = PA_Summer[['Smoking', 'Drinking', 'Short_Sleep', 'Physical_Activity', 'neighbor_avg_density',\n                      'minority', 'aging', 'disability']]\nX_train_scaled, X_test_scaled, y_train, y_test_su, scaler = split_and_scale_data(X_Summer_Sol, y_Summer, stratify_col)\nsummer_result_sol = train_and_evaluate_rf(X_train_scaled, X_test_scaled, y_train, y_test_su, X_Summer_Sol.columns)"
  },
  {
    "objectID": "code/Machine_Learning.html#winter-model",
    "href": "code/Machine_Learning.html#winter-model",
    "title": "Running Machine Learning Model",
    "section": "Winter Model",
    "text": "Winter Model\n\n# using all variables\nstratify_col = PA_Winter['CountyName']\nX_Winter_All = PA_Winter[['Smoking', 'Drinking', 'Short_Sleep', 'Physical_Activity', 'neighbor_avg_density',\n                    'minority', 'aging', 'disability', 'forest_total', 'wetland_total', 'Hdensity_total',\n                    'Ldensity_total', 'grasses_total', 'water_total', 'forest_pct',  'Hdensity_pct',\n                    'evi_winter','lst_winter', 'ndvi_winter', 'savi_winter']]\ny_Winter = PA_Winter[['winter_count']]\nX_train_scaled, X_test_scaled, y_train, y_test_wi, scaler = split_and_scale_data(X_Winter_All, y_Winter, stratify_col)\nwinter_result = train_and_evaluate_rf(X_train_scaled, X_test_scaled, y_train, y_test_wi, X_Winter_All.columns)\n\n\n# using all variables after dimensionality reduction\nX_Winter_BIC = PA_Winter[['Smoking', 'Drinking', 'Short_Sleep', 'Physical_Activity', 'minority', 'aging', 'disability', 'forest_total', 'Ldensity_total', 'grasses_total', 'forest_pct', 'lst_winter']]\nX_train_scaled, X_test_scaled, y_train, y_test_wi, scaler = split_and_scale_data(X_Winter_BIC, y_Winter, stratify_col)\nwinter_result_BIC = train_and_evaluate_rf(X_train_scaled, X_test_scaled, y_train, y_test_wi, X_Winter_BIC.columns)\n\n\n# using only environment variables\nX_Winter_Env = PA_Winter[['forest_total', 'wetland_total', 'Hdensity_total',\n                    'Ldensity_total', 'grasses_total', 'water_total', 'forest_pct',  'Hdensity_pct',\n                    'evi_winter','lst_winter', 'ndvi_winter', 'savi_winter']]\nX_train_scaled, X_test_scaled, y_train, y_test_wi, scaler = split_and_scale_data(X_Winter_Env, y_Winter, stratify_col)\nwinter_result_env = train_and_evaluate_rf(X_train_scaled, X_test_scaled, y_train, y_test_wi, X_Winter_Env.columns)\n\n\n# using only social behavorial variables\nX_Winter_Sol = PA_Winter[['Smoking', 'Drinking', 'Short_Sleep', 'Physical_Activity', 'neighbor_avg_density',\n                      'minority', 'aging', 'disability']]\nX_train_scaled, X_test_scaled, y_train, y_test_wi, scaler = split_and_scale_data(X_Winter_Sol, y_Winter, stratify_col)\nwinter_result_sol = train_and_evaluate_rf(X_train_scaled, X_test_scaled, y_train, y_test_wi, X_Winter_Sol.columns)\n\n\nmethods = ['sp_all', 'sp_bic', 'sp_env', 'sp_sol',\n           'su_all', 'su_bic', 'su_env', 'su_sol',\n           'fa_all', 'fa_bic', 'fa_env', 'fa_sol',\n           'wi_all', 'wi_bic', 'wi_env', 'wi_sol']\nresults = [spring_result, spring_result_BIC, spring_result_env, spring_result_sol,\n           summer_result, summer_result_BIC, summer_result_env, summer_result_sol,\n           fall_result, fall_result_BIC, fall_result_env, fall_result_sol,\n           winter_result, winter_result_BIC, winter_result_env, winter_result_sol\n           ]\n\nmetrics_data = {}\ntop_features_data = {}\n\nfor method, result in zip(methods, results):\n    metrics, top_features = extract_results(result, method)\n    metrics_data[method] = metrics\n    top_features_data[method] = top_features\n\ncompare_metrics = pd.DataFrame(metrics_data, index=['RMSE', 'R2'])\ncomapre_top_features = pd.DataFrame(top_features_data, index=[f'Feature {i+1}' for i in range(5)])\n\n\ncompare_metrics\n\n\n  \n    \n\n\n\n\n\n\nsp_all\nsp_bic\nsp_env\nsp_sol\nsu_all\nsu_bic\nsu_env\nsu_sol\nfa_all\nfa_bic\nfa_env\nfa_sol\nwi_all\nwi_bic\nwi_env\nwi_sol\n\n\n\n\nRMSE\n22.215634\n23.125607\n25.363977\n28.684670\n16.041516\n17.048896\n18.873498\n20.608387\n20.240806\n21.529757\n23.914098\n26.085645\n26.510291\n28.301329\n31.487773\n33.886914\n\n\nR2\n0.561915\n0.525291\n0.428948\n0.269634\n0.557143\n0.499776\n0.386977\n0.269096\n0.559970\n0.502143\n0.385765\n0.269148\n0.553349\n0.490958\n0.369879\n0.270200\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n\ncomapre_top_features\n\n\n  \n    \n\n\n\n\n\n\nsp_all\nsp_bic\nsp_env\nsp_sol\nsu_all\nsu_bic\nsu_env\nsu_sol\nfa_all\nfa_bic\nfa_env\nfa_sol\nwi_all\nwi_bic\nwi_env\nwi_sol\n\n\n\n\nFeature 1\nHdensity_total\nLdensity_total\nHdensity_total\nPhysical_Activity\nHdensity_total\nLdensity_total\nHdensity_total\nPhysical_Activity\nHdensity_total\nLdensity_total\nHdensity_total\nPhysical_Activity\nHdensity_total\nLdensity_total\nHdensity_total\nPhysical_Activity\n\n\nFeature 2\nLdensity_total\nPhysical_Activity\nLdensity_total\nneighbor_avg_density\nLdensity_total\nPhysical_Activity\nLdensity_total\nneighbor_avg_density\nLdensity_total\nPhysical_Activity\nLdensity_total\nneighbor_avg_density\nLdensity_total\nPhysical_Activity\nLdensity_total\nneighbor_avg_density\n\n\nFeature 3\nPhysical_Activity\nlst_spring\nlst_spring\ndisability\nPhysical_Activity\nlst_summer\nndvi_summer\ndisability\nPhysical_Activity\nlst_fall\nHdensity_pct\ndisability\nPhysical_Activity\nforest_total\nHdensity_pct\ndisability\n\n\nFeature 4\nneighbor_avg_density\nforest_total\nHdensity_pct\naging\nneighbor_avg_density\nSmoking\nlst_summer\naging\nneighbor_avg_density\nSmoking\nlst_fall\naging\nneighbor_avg_density\nSmoking\nlst_winter\naging\n\n\nFeature 5\nSmoking\nSmoking\nndvi_spring\nSmoking\nSmoking\nminority\nHdensity_pct\nSmoking\nSmoking\nforest_total\nndvi_fall\nSmoking\nSmoking\nminority\nndvi_winter\nSmoking\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n\ncompare_metrics.to_csv('/content/drive/MyDrive/compare_metrics.csv')\ncomapre_top_features.to_csv('/content/drive/MyDrive/comapre_top_features.csv')\n\n\n# Create a scatter plot\nplt.scatter(spring_result['predictions'], y_test_sp, alpha=0.7, color='blue', edgecolor='black')\nplt.title('Scatter Plot of Predictions vs Spring Count')\nplt.xlabel('Predictions')\nplt.ylabel('Spring Count')\nplt.grid(True, linestyle='--', alpha=0.6)\nplt.show()"
  },
  {
    "objectID": "code/Data_Preparation.html",
    "href": "code/Data_Preparation.html",
    "title": "Large Dataset Loading and Preparation Scripts",
    "section": "",
    "text": "Just for demonstration purposes, do not run!\nimport ee\nimport geemap\nimport os\nimport requests\nfrom datetime import datetime\nimport geopandas as gpd\nimport pandas as pd\nimport cenpy\nimport pygris\nimport warnings\nwarnings.filterwarnings('ignore')"
  },
  {
    "objectID": "code/Data_Preparation.html#preparation",
    "href": "code/Data_Preparation.html#preparation",
    "title": "Large Dataset Loading and Preparation Scripts",
    "section": "1. Preparation",
    "text": "1. Preparation\n\nee.Authenticate()\nee.Initialize(project=\"gee-emilyzhou0112\") # replace with your own project name\n\n\npa_tracts = gpd.read_file('PATH')\npa_bound = pa_tracts.dissolve() # dissolve geometry to get the boundary\npa_geom= ee.Geometry.Polygon(list(pa_bound['geometry'].iloc[0].exterior.coords)) # convert the geometry into a format suitable for gee\naoi = ee.FeatureCollection(pa_geom)\n\n\ntolerance = 0.01\npa_tracts['geometry'] = pa_tracts['geometry'].simplify(tolerance, preserve_topology=True)\npa_tracts_ee = geemap.geopandas_to_ee(pa_tracts)"
  },
  {
    "objectID": "code/Data_Preparation.html#landsat-data",
    "href": "code/Data_Preparation.html#landsat-data",
    "title": "Large Dataset Loading and Preparation Scripts",
    "section": "2. Landsat Data",
    "text": "2. Landsat Data\n\n## Define Time Period\nstartSpring = datetime(2022, 3, 1) # spring\nendSpring = datetime(2022, 5, 31)\nstartSummer = datetime(2022, 6, 1) # summer\nendSummer = datetime(2022, 8, 31)\nstartFall = datetime(2022, 9, 1) # fall\nendFall = datetime(2022, 11, 30)\nstartWinter = datetime(2022, 12, 1) # winter\nendWinter = datetime(2023, 2, 28)\n\n# Format dates into strings that Earth Engine expects (\"YYYY-MM-DD\")\nstartSpring= startSpring.strftime('%Y-%m-%d')\nendSpring = endSpring.strftime('%Y-%m-%d')\nstartSummer = startSummer.strftime('%Y-%m-%d')\nendSummer = endSummer.strftime('%Y-%m-%d')\nstartFall = startFall.strftime('%Y-%m-%d')\nendFall = endFall.strftime('%Y-%m-%d')\nstartWinter = startWinter.strftime('%Y-%m-%d')\nendWinter = endWinter.strftime('%Y-%m-%d')\n\n\n## Helper Function - Scale Bands\ndef apply_scale_factors(image):\n    # Scale and offset values for optical bands\n    optical_bands = image.select('SR_B.').multiply(0.0000275).add(-0.2)\n\n    # Scale and offset values for thermal bands\n    thermal_bands = image.select('ST_B.*').multiply(0.00341802).add(149.0)\n\n    # Add scaled bands to the original image\n    return image.addBands(optical_bands, None, True) \\\n                .addBands(thermal_bands, None, True)\n\n\n## Helper Function - Mask Clouds\ndef cloud_mask(image):\n    # Define cloud shadow and cloud bitmask (Bits 3 and 5)\n    cloud_shadow_bit_mask = 1 &lt;&lt; 3\n    cloud_bit_mask = 1 &lt;&lt; 5\n\n    # Select the Quality Assessment (QA) band for pixel quality information\n    qa = image.select('QA_PIXEL')\n\n    # Create a binary mask to identify clear conditions (both cloud and cloud shadow bits set to 0)\n    mask = qa.bitwiseAnd(cloud_shadow_bit_mask).eq(0) \\\n                .And(qa.bitwiseAnd(cloud_bit_mask).eq(0))\n\n    # Update the original image, masking out cloud and cloud shadow-affected pixels\n    return image.updateMask(mask)\n\n\ndef calculate_seasonal_indices(image_collection, aoi, season_name):\n    \"\"\"\n    Calculate NDVI, SAVI, EVI, Fraction of Vegetation (FV),\n    Emissivity (EM), and Land Surface Temperature (LST) for a season.\n\n    Parameters:\n    - image_collection: ee.ImageCollection, the collection of images for the season.\n    - aoi: ee.Geometry, the area of interest.\n    - season_name: str, name of the season (for debugging/logging purposes).\n\n    Returns:\n    - ee.Image, containing the calculated indices and LST.\n    \"\"\"\n\n    # Calculate NDVI\n    ndvi = image_collection.normalizedDifference(['SR_B5', 'SR_B4']).rename('NDVI')\n\n    # Calculate SAVI\n    savi = image_collection.expression(\n        '1.5 * (NIR - RED) / (NIR + RED + 0.5)', {\n            'NIR': image_collection.select('SR_B5'),\n            'RED': image_collection.select('SR_B4')\n        }\n    ).rename('SAVI')\n\n    # Calculate EVI\n    evi = image_collection.expression(\n        '2.5 * ((NIR - RED) / (NIR + 6 * RED - 7.5 * BLUE + 1))', {\n            'NIR': image_collection.select('SR_B5'),\n            'RED': image_collection.select('SR_B4'),\n            'BLUE': image_collection.select('SR_B2')\n        }\n    ).rename('EVI')\n\n    # NDVI min and max for Fraction of Vegetation (FV) calculation\n    ndvi_min = ndvi.reduceRegion(\n        reducer=ee.Reducer.min(),\n        geometry=aoi,\n        scale=30,\n        maxPixels=1e9\n    ).get('NDVI')\n\n    ndvi_max = ndvi.reduceRegion(\n        reducer=ee.Reducer.max(),\n        geometry=aoi,\n        scale=30,\n        maxPixels=1e9\n    ).get('NDVI')\n\n    # Convert NDVI_min and NDVI_max to ee.Number\n    ndvi_min = ee.Number(ndvi_min)\n    ndvi_max = ee.Number(ndvi_max)\n\n    # Fraction of Vegetation (FV)\n    fv = ndvi.subtract(ndvi_min).divide(ndvi_max.subtract(ndvi_min)).pow(2).rename('FV')\n\n    # Emissivity (EM)\n    em = fv.multiply(0.004).add(0.986).rename('EM')\n\n    # Thermal band (Band 10)\n    thermal = image_collection.select('ST_B10').rename('thermal')\n\n    # Land Surface Temperature (LST)\n    lst = thermal.expression(\n        '(TB / (1 + (0.00115 * (TB / 1.438)) * log(em))) - 273.15',\n        {\n            'TB': thermal.select('thermal'),  # Thermal band temperature in Kelvin\n            'em': em  # Emissivity\n        }\n    ).rename('LST')\n\n    seasonal_image = ndvi.addBands([savi, evi, fv, em, lst])\n    return seasonal_image\n\n\nseasons = {\n    'spring': (startSpring, endSpring),\n    'summer': (startSummer, endSummer),\n    'fall': (startFall, endFall),\n    'winter': (startWinter, endWinter)\n}\n\nseasonal_results = {}\nfor season, (start_date, end_date) in seasons.items():\n    image_collection = ee.ImageCollection(\"LANDSAT/LC08/C02/T1_L2\") \\\n        .filterBounds(aoi) \\\n        .filterDate(start_date, end_date) \\\n        .map(apply_scale_factors) \\\n        .map(cloud_mask) \\\n        .median() \\\n        .clip(aoi)\n\n    seasonal_results[season] = calculate_seasonal_indices(image_collection, aoi, season)\n\n\n# Function to export zonal stats to Google Drive\ndef export_zonal_stats(image, reducer, file_name, folder_name=\"FILE NAME\"):\n    \"\"\"\n    Exports zonal statistics of an image band to Google Drive as a CSV.\n\n    Parameters:\n    - image: ee.Image, the image containing the band to export.\n    - reducer: ee.Reducer, the reducer to aggregate data (e.g., mean, median).\n    - file_name: str, name of the file (e.g., 'ndvi_spring.csv').\n    - folder_name: str, Google Drive folder to save the file in.\n    \"\"\"\n    zonal_stats = image.reduceRegions(\n        collection=pa_tracts_ee,\n        reducer = ee.Reducer.mean()\n        scale=30  # Resolution of the analysis\n    )\n\n    task = ee.batch.Export.table.toDrive(\n        collection=zonal_stats,\n        fileFormat='CSV',\n        fileNamePrefix=file_name.replace('.csv', ''),\n        folder=folder_name\n    )\n    task.start()\n    print(f\"Export started for {file_name}. Check Google Drive for the results.\")\n\n\n# Seasonal results containing all seasonal images with bands\nseasonal_results = {\n    \"spring\": seasonal_results['spring'],\n    \"summer\": seasonal_results['summer'],\n    \"fall\": seasonal_results['fall'],\n    \"winter\": seasonal_results['winter']\n}\n\n# List of bands to process\nbands = ['NDVI', 'EVI', 'SAVI', 'LST']\n\n# Export each band for every season\nfor season, image in seasonal_results.items():\n    for band in bands:\n        band_image = image.select(band)  # Extract specific band\n        file_name = f\"{band.lower()}_{season}.csv\"  # File name e.g., ndvi_spring.csv\n        export_zonal_stats(image=band_image, reducer=reducer, file_name=file_name)"
  },
  {
    "objectID": "code/Data_Preparation.html#land-cover-data",
    "href": "code/Data_Preparation.html#land-cover-data",
    "title": "Large Dataset Loading and Preparation Scripts",
    "section": "3. Land Cover Data",
    "text": "3. Land Cover Data\n\ndataset = ee.ImageCollection('USGS/NLCD_RELEASES/2021_REL/NLCD')\nnlcd2021 = dataset.filter(ee.Filter.eq('system:index', '2021')).first()\nlandcover = nlcd2021.select('landcover')\npa_landcover = landcover.clip(aoi)\n\n\nhigh_density = pa_landcover.eq(23).Or(pa_landcover.eq(24))\nlow_density = pa_landcover.eq(21).Or(pa_landcover.eq(22))\nforest = pa_landcover.eq(41).Or(pa_landcover.eq(42)).Or(pa_landcover.eq(43))\ngrasses = pa_landcover.eq(52).Or(pa_landcover.eq(71)).Or(pa_landcover.eq(81)).Or(pa_landcover.eq(82))\nwetlands = pa_landcover.eq(90).Or(pa_landcover.eq(95))\nopen_water = pa_landcover.eq(11)\n\n\ndef neighboring_landcover_metrics(landcover_mask, file_name):\n    \"\"\"\n    Function to calculate total and neighboring land cover metrics and export them as a CSV.\n\n    Args:\n        landcover_mask (ee.Image): Binary mask of the landcover categories to analyze.\n        pa_tracts_ee (ee.FeatureCollection): The census tracts FeatureCollection.\n        description (str): Description for the export task.\n        file_name (str): File name prefix for the exported CSV.\n    \"\"\"\n    # Define the kernel for neighboring pixels\n    kernel = ee.Kernel.square(radius=1, units='pixels')  # 3x3 neighborhood\n    neighbors = landcover_mask.convolve(kernel).gte(1)  # At least one neighbor\n\n    # Calculate total landcover pixels\n    total_landcover = landcover_mask.reduceRegions(\n        collection=pa_tracts_ee,\n        reducer=ee.Reducer.sum(),\n        scale=30\n    ).select(['sum'], ['total_landcover'])\n\n    # Calculate neighboring landcover pixels\n    neighbor_landcover = neighbors.reduceRegions(\n        collection=pa_tracts_ee,\n        reducer=ee.Reducer.sum(),\n        scale=30\n    ).select(['sum'], ['neighbor_landcover'])\n\n    # Merge FeatureCollections and retain geoid\n    merged_fc = total_landcover.map(lambda feature:\n        feature.set(\n            'neighbor_landcover',\n            neighbor_landcover.filter(ee.Filter.eq('system:index', feature.get('system:index')))\n                              .first()\n                              .get('neighbor_landcover')\n        ).set(\n            'geoid', pa_tracts_ee.filter(ee.Filter.eq('system:index', feature.get('system:index')))\n                                 .first()\n                                 .get('GEOID')\n        )\n    )\n\n    # Export the merged FeatureCollection\n    export_task = ee.batch.Export.table.toDrive(\n        collection=merged_fc.select(['geoid', 'total_landcover', 'neighbor_landcover']),\n        folder='FOLDER NAME',\n        fileNamePrefix=file_name,\n        fileFormat='CSV'\n    )\n    export_task.start()\n    print(f\"Export task started: {file_name}\")\n\n\nneighboring_landcover_metrics(\n    landcover_mask=forest,\n    file_name='forest_landcover_metrics'\n)\n\nneighboring_landcover_metrics(\n    landcover_mask=high_density,\n    file_name='high_density_landcover_metrics'\n)\n\n\ndef summarize_landcover_pixels(landcover_mask, file_name):\n    \"\"\"\n    Function to summarize total landcover pixels for each tract and export as a CSV.\n\n    Args:\n        landcover_mask (ee.Image): Binary mask of the landcover categories to analyze.\n        pa_tracts_ee (ee.FeatureCollection): The census tracts FeatureCollection.\n        description (str): Description for the export task.\n        file_name (str): File name prefix for the exported CSV.\n    \"\"\"\n    # Calculate total landcover pixels\n    total_landcover = landcover_mask.reduceRegions(\n        collection=pa_tracts_ee,\n        reducer=ee.Reducer.sum(),\n        scale=30\n    ).map(lambda feature: feature.set(\n        'geoid', feature.get('GEOID')\n    ))\n\n    # Export the results to Drive\n    export_task = ee.batch.Export.table.toDrive(\n        collection=total_landcover.select(['geoid', 'sum']),\n        folder='FOLDER NAME',\n        fileNamePrefix=file_name,\n        fileFormat='CSV'\n    )\n    export_task.start()\n    print(f\"Export task started: {file_name}\")\n\n\nlandcover_list = [\n    {'mask': grasses, 'file_name': 'grasses_landcover'},\n    {'mask': low_density, 'file_name': 'low_density_landcover'},\n    {'mask': wetlands, 'file_name': 'wetlands_landcover'},\n    {'mask': open_water, 'file_name': 'open_water_landcover'}\n]\n\nfor landcover in landcover_list:\n    summarize_landcover_pixels(landcover['mask'], landcover['file_name'])"
  },
  {
    "objectID": "code/Data_Preparation.html#tobacco-retailer-data",
    "href": "code/Data_Preparation.html#tobacco-retailer-data",
    "title": "Large Dataset Loading and Preparation Scripts",
    "section": "4. Tobacco Retailer Data",
    "text": "4. Tobacco Retailer Data\n\nall_retailers = pd.read_csv('PATH')\npa_retailers = all_retailers[all_retailers['state'] == 'PA']\npa_retailers = pa_retailers[[\"county\", \"license_type\", \"lat\", \"lon\"]]\n\n\npa_retailers.to_csv('PATH', index=False)"
  },
  {
    "objectID": "code/Data_Preparation.html#cdc-data",
    "href": "code/Data_Preparation.html#cdc-data",
    "title": "Large Dataset Loading and Preparation Scripts",
    "section": "5. CDC Data",
    "text": "5. CDC Data\n\ncdc_data = pd.read_csv(\"PATH\")\n\n\n# process CRD data\nPA_Asthma = cdc_data[(cdc_data['Measure'] == \"Current asthma among adults\") & (cdc_data['StateAbbr'] == \"PA\")]\nPA_COP = cdc_data[(cdc_data['Measure'] == \"Chronic obstructive pulmonary disease among adults\") & (cdc_data['StateAbbr'] == \"PA\")]\nPA_Chronic = PA_Asthma.merge(\n    PA_COP[['LocationName', 'Data_Value']],\n    on=\"LocationName\",\n    how=\"left\"\n).rename(columns={\"Data_Value_x\": \"Asthma\", \"Data_Value_y\": \"COP\"})\n\n\nPA_Chronic.to_csv('PATH', index=False)\n\n\n# process HRB data\nPA_Smoking = cdc_data[(cdc_data['Measure'] == \"Current cigarette smoking among adults\") & (cdc_data['StateAbbr'] == \"PA\")]\nPA_Drinking = cdc_data[(cdc_data['Measure'] == \"Binge drinking among adults\") & (cdc_data['StateAbbr'] == \"PA\")]\nPA_Physical_Activity = cdc_data[(cdc_data['Measure'] == \"No leisure-time physical activity among adults\") & (cdc_data['StateAbbr'] == \"PA\")]\nPA_Short_Sleep = cdc_data[(cdc_data['Measure'] == \"Short sleep duration among adults\") & (cdc_data['StateAbbr'] == \"PA\")]\n\nPA_HRB = PA_Smoking.merge(\n    PA_Drinking[['LocationName', 'Data_Value']], on='LocationName', how='left'\n).rename(columns={\"Data_Value_x\": \"Smoking\", \"Data_Value_y\": \"Drinking\"})\n\nPA_HRB = PA_HRB.merge(\n    PA_Physical_Activity[['LocationName', 'Data_Value']], on='LocationName', how='left'\n).rename(columns={'Data_Value': 'Physical_Activity'})\n\nPA_HRB = PA_HRB.merge(\n    PA_Short_Sleep[['LocationName', 'Data_Value']], on='LocationName', how='left'\n).rename(columns={'Data_Value': 'Short_Sleep'})\nPA_HRB[['LocationName', 'Smoking', 'Drinking', 'Physical_Activity', 'Short_Sleep']]\n\n\nPA_HRB.to_csv('PATH', index=False)"
  },
  {
    "objectID": "code/Data_Preparation.html#census-data",
    "href": "code/Data_Preparation.html#census-data",
    "title": "Large Dataset Loading and Preparation Scripts",
    "section": "6. Census Data",
    "text": "6. Census Data\n\nacs = cenpy.remote.APIConnection(\"ACSDT5Y2022\")\n\n\ncensus_var = [\"NAME\",\n              \"B02001_001E\", # total\n              \"B02001_002E\", # white\n              \"B02001_003E\", # black\n              \"B02001_004E\", # native american\n              \"B02001_005E\", # asian\n              \"B03002_012E\", # hispanic\n              'B01001_020E', # male 65-66\n              'B01001_021E', # male 67-69\n              'B01001_022E', # male 70-74\n              'B01001_023E', # male 75-79\n              'B01001_024E', # male 80-84\n              'B01001_025E', # male over 85\n              'B01001_044E', # female 65-66\n              'B01001_045E', # female 67-69\n              'B01001_046E', # female 70-74\n              'B01001_047E', # female 75-79\n              'B01001_048E', # female 80-84\n              'B01001_049E', # female over 85\n              'B18101_007E', # Male 5 to 17 years With a disability\n              'B18101_010E', # Male 18 to 34 years With a disability\n              'B18101_013E', # Male 35 to 64 years With a disability\n              'B18101_016E', # Male 65 to 74 years With a disability\n              'B18101_019E', # Male over 75 years With a disability\n              'B18101_026E', # Female 5 to 17 years With a disability\n              'B18101_029E', # Female 18 to 34 years With a disability\n              'B18101_032E', # Female 35 to 64 years With a disability\n              'B18101_035E', # Female 65 to 74 years With a disability\n              'B18101_038E'\n             ]\n\n\npa_state_code = \"42\"\ncensus_data = acs.query(\n    cols=census_var,\n    geo_unit=\"tract\",\n    geo_filter={\"state\": pa_state_code}\n)\nfor variable in census_var:\n    if variable != \"NAME\":\n        census_data[variable] = census_data[variable].astype(float)\n\n\ncensus_data['minority'] = (\n    (census_data['B02001_001E'] - census_data['B02001_002E']) / census_data['B02001_001E']\n)\ncensus_data['aging'] = (\n    census_data[[\n        'B01001_020E', 'B01001_021E', 'B01001_022E', 'B01001_023E',\n        'B01001_024E', 'B01001_025E', 'B01001_044E', 'B01001_045E',\n        'B01001_046E', 'B01001_047E', 'B01001_048E', 'B01001_049E'\n    ]].sum(axis=1) / census_data['B02001_001E']\n)\ncensus_data['disability'] = (\n    census_data[[\n        'B18101_007E', 'B18101_010E', 'B18101_013E', 'B18101_016E',\n        'B18101_019E', 'B18101_026E', 'B18101_029E', 'B18101_032E',\n        'B18101_035E', 'B18101_038E'\n    ]].sum(axis=1) / census_data['B02001_001E']\n)\n\n\ncensus_data = census_data[[\"NAME\", \"county\", \"tract\", \"minority\", \"aging\", \"disability\"]]\ntracts = pygris.tracts(state=pa_state_code, year=2022)\npa_census_data = tracts.merge(census_data, left_on=[\"COUNTYFP\", \"TRACTCE\"], right_on=[\"county\", \"tract\"],)\npa_census_data = pa_census_data[[\"GEOID\", \"minority\", \"aging\", \"disability\", \"geometry\"]]\n\n\npa_census_data.to_csv('PATH', index=False)"
  },
  {
    "objectID": "analysis/PrepareML.html",
    "href": "analysis/PrepareML.html",
    "title": "Machine Learning Data Preparation",
    "section": "",
    "text": "The purpose of this section of the methodology is to examine the loaded data, explore its structure, and perform additional processing to prepare the variables needed for machine learning models. The workflow merges all relevant variables into a single cohesive dataframe. By the end of the notebook, the output is a unified dataframe containing approximately 3,000 rows, each representing a census tract.\nThis notebook can be executed by loading the processed data from our GitHub repository. To reproduce our analysis, the following Python libraries are required: pandas for data manipulation, geopandas for handling spatial data, numpy for numerical operations, shapely for geometric analysis, and libpysal for spatial statistics and spatial data processing. Ensure that libpysal is installed to replicate our results accurately.\nCode\nimport os\nimport pandas as pd\nimport numpy as np\nimport geopandas as gpd\nimport matplotlib.pyplot as plt\nfrom shapely.geometry import shape, Polygon, Point\nfrom libpysal.weights import Queen\nfrom libpysal.weights.spatial_lag import lag_spatial",
    "crumbs": [
      "Methodology",
      "Machine Learning Data Preparation"
    ]
  },
  {
    "objectID": "analysis/PrepareML.html#pa-geometry",
    "href": "analysis/PrepareML.html#pa-geometry",
    "title": "Machine Learning Data Preparation",
    "section": "PA Geometry",
    "text": "PA Geometry\nLet’s first check the geometry of Pennsylvania’s census tracts. After processing, the dataset is simplified to contain only two columns: GEOID and geometry. GEOID is useful when performing plain join to other variables while geometry will be useful when we compute nearest neighbor.\n\n\nCode\nPA_Tracts.head(3)\n\n\n\n\n\n\n\n\n\nGEOID\ngeometry\n\n\n\n\n0\n42001030101\nPOLYGON ((-77.15558 40.05671, -77.15467 40.057...\n\n\n1\n42001030103\nPOLYGON ((-77.07256 39.97480, -77.07223 39.974...\n\n\n2\n42001030104\nPOLYGON ((-77.09913 39.94784, -77.09750 39.948...\n\n\n\n\n\n\n\nThe plot below quickly visualizes census tracts in Pennsylvania.",
    "crumbs": [
      "Methodology",
      "Machine Learning Data Preparation"
    ]
  },
  {
    "objectID": "analysis/PrepareML.html#copd-data-and-health-risk-behaviors",
    "href": "analysis/PrepareML.html#copd-data-and-health-risk-behaviors",
    "title": "Machine Learning Data Preparation",
    "section": "COPD Data and Health Risk Behaviors",
    "text": "COPD Data and Health Risk Behaviors\nNow, let’s check both the COPD data and the health risks behavior data. After the cleaning process, the COPD dataset would only include 6 columns, among which we are mostly interested in LocationName (the GEOID) used to join it with other dataset as well as the prevalence of COPD.\n\n\nCode\nPA_Chronic.head(3)\n\n\n\n\n\n\n\n\n\nCountyName\nLocationName\nAsthma\nTotalPopulation\nTotalPop18plus\nCOP\n\n\n\n\n0\nAllegheny\n42003141200\n9.7\n4007\n3242\n4.8\n\n\n1\nAllegheny\n42003140100\n10.6\n5579\n5066\n4.2\n\n\n2\nAllegheny\n42003191900\n10.6\n2177\n1786\n5.9\n\n\n\n\n\n\n\nA quick visualization of COPD prevalence in Pennsylvania reveals that urban areas such as Pittsburgh and Philadelphia experience higher COPD prevalence compare to the suburban area in its vincinity. However, several rural areas in Northern and Central Pennsylvania exhibit much higher rate of COPD.\n\n\n\n\n\n\n\n\n\nIndeed, if we look at the top 10 counties in Pennsylvania with highest average COPD prevalence, we many notice that these are not urban counties. They are also not proximate to any urban areas. Cameron, Mifflin, Forest, and Sullivan counties are both located in central Pennsylvania. This is a pretty surprising observation as researchers often assume people living in urban areas to be more vulnerable to COPD because of air pollution, development density, etc.\n\n\n\n\n\n\n\n\n\nThe health risks behavior dataset includes only 6 variables as well, which are mainly the percentages of population who smoke and drink on a regular basis and having short sleep, no physical activity.\n\n\nCode\nPA_HRB.head(3)\n\n\n\n\n\n\n\n\n\nCountyName\nLocationName\nSmoking\nDrinking\nShort_Sleep\nPhysical_Activity\n\n\n\n\n0\nAdams\n42001030300\n17.7\n18.2\n39.6\n25.4\n\n\n1\nAllegheny\n42003151700\n15.2\n21.2\n34.8\n20.8\n\n\n2\nAllegheny\n42003151600\n17.1\n21.7\n35.1\n21.8",
    "crumbs": [
      "Methodology",
      "Machine Learning Data Preparation"
    ]
  },
  {
    "objectID": "analysis/PrepareML.html#tobacco-retailer-data",
    "href": "analysis/PrepareML.html#tobacco-retailer-data",
    "title": "Machine Learning Data Preparation",
    "section": "Tobacco Retailer Data",
    "text": "Tobacco Retailer Data\nAfter loading in the retailers data PA_Retailers, we need to create a new column geometry by applying a lambda function to each row. The lambda function constructs a Point object using the longitude (lon) and latitude (lat) values from each row. The next piece of code convert the PA_Retailers DataFrame into a GeoDataFrame, specifying the geometry column and setting the coordinate reference system (CRS) to EPSG:4326.\n\n# make retailer data spatial\nPA_Retailers['geometry'] = PA_Retailers.apply(\n    lambda row: Point(row['lon'], row['lat']), axis=1\n)\nPA_Retailers = gpd.GeoDataFrame(\n    PA_Retailers,\n    geometry='geometry',\n    crs='EPSG:4326'\n)\n\nWe included here a quick visualization of the distribution of tobacco retailers in Pennsylvania.\n\n\n\n\n\n\n\n\n\nSince in the introduction, we’ve mentioned that we would like to account for environment exposures to tobacco retailers from neighboring tracts considering that people travel between different tracts everyday, there’s some extrat steps we need to do to compute retailer density and average them across it neighbors. The code below first performs a spatial join between PA_Tracts and PA_Retailers, joining the two GeoDataFrames based on their spatial relationship. The how=“left” parameter ensures that all tracts are retained, and the predicate=“intersects” parameter specifies that retailers intersecting with tracts should be joined. Then, we need to summarize the total retailers in each tract, and merge the summary beack to the original PA_Tracts GeoDataFrame. Any tracts without retailers are filled with a count of zero. To compute density, we also need to convert our GeoDataFrame to EPSG:3857 for more accurate area calculations. Then, the density of retailers is calcualted through dividing the total number of retailers by the area of the tract.\n\n# count retailers\ntracts_with_retailers = gpd.sjoin(PA_Tracts, PA_Retailers, how=\"left\", predicate=\"intersects\")\ntracts_summary = (\n    tracts_with_retailers.groupby(\"GEOID\")\n    .size()\n    .reset_index(name=\"total_retailers\")\n)\n\ntracts_with_retailers= PA_Tracts.merge(tracts_summary, on=\"GEOID\", how=\"left\")\ntracts_with_retailers[\"total_retailers\"] = tracts_with_retailers[\"total_retailers\"].fillna(0)\n\n# compute density\ndensity = tracts_with_retailers.to_crs(epsg=3857)\ndensity['area_km2'] = density.geometry.area / 1e6  # Convert m² to km²\ndensity ['density'] = density ['total_retailers'] / density ['area_km2']\n\nTo know the average density across neighboring tracts, a spatial weights matrix is created based on the geometries of tracts using the Queen contiguity method, which considers tracts as neighbors if they share a boundary or a vertex. This is done with the Queen.from_dataframe(density) function. Next, it computes the average density of neighboring tracts by applying the lag_spatial function, which uses the spatial weights matrix to calculate the spatial lag of the density column, resulting in a new column neighbor_avg_density in the density GeoDataFrame. Finally, the code removes the area_km2 column from the density GeoDataFrame, renaming it to PA_Retailers.\n\n# Create spatial weights matrix based on tract geometries\nweights = Queen.from_dataframe(density)\n\n# Compute average density of neighbors using lag_spatial\ndensity['neighbor_avg_density'] = lag_spatial(\n    weights, density['density']\n)\nPA_Retailers = density.drop(columns=[\"area_km2\"])\n\nThe final PA_Retailers dataset would only include the following columns in addition to GEOID and the geometry: total retailers of a census tract, retailer density, as well as the average density across its neighboring tracts.\n\n\nCode\nPA_Retailers.head(5)\n\n\n\n\n\n\n\n\n\nGEOID\ngeometry\ntotal_retailers\ndensity\nneighbor_avg_density\n\n\n\n\n0\n42001030101\nPOLYGON ((-8588920.357 4874186.492, -8588818.5...\n3\n0.031552\n0.227339\n\n\n1\n42001030103\nPOLYGON ((-8579678.266 4862280.523, -8579641.3...\n1\n0.031470\n0.158547\n\n\n2\n42001030104\nPOLYGON ((-8582635.361 4858365.994, -8582454.6...\n7\n0.081237\n1.671000\n\n\n3\n42001030200\nPOLYGON ((-8599802.938 4869004.417, -8599007.8...\n8\n0.038641\n0.413897\n\n\n4\n42001030300\nPOLYGON ((-8618506.453 4863224.423, -8618215.5...\n5\n0.026165\n0.216389\n\n\n\n\n\n\n\nThe visualization of average tobacco retailer density across neighboring census tract is not very exciting, as expected, since most of the retailers are located in urban areas. Except for Philadelphia and Pittsburgh, where the density of tobacco retailer is high, most rurual areas in Pennsylvania has a low tobacco retailer density. This suggets that exposure to tobacco retailer store might not have a significant effect to explain the high COPD in central and northern Pennsylvania.",
    "crumbs": [
      "Methodology",
      "Machine Learning Data Preparation"
    ]
  },
  {
    "objectID": "analysis/PrepareML.html#census-data",
    "href": "analysis/PrepareML.html#census-data",
    "title": "Machine Learning Data Preparation",
    "section": "Census Data",
    "text": "Census Data\nNow, let’s take a look at the census data. The processed dataset only contains the predictors we will use in machine leanring models: percentage of minority, aging and disability population.\n\n\nCode\nPA_Census.head(3)\n\n\n\n\n\n\n\n\n\nGEOID\nminority\naging\ndisability\ngeometry\n\n\n\n\n0\n42125775200\n0.211840\n0.215264\n0.287671\nPOLYGON ((-79.876586 40.177549, -79.876234 40....\n\n\n1\n42125775300\n0.395709\n0.131506\n0.273739\nPOLYGON ((-79.879294 40.174857, -79.878454 40....\n\n\n2\n42125782700\n0.048652\n0.314267\n0.302433\nPOLYGON ((-79.913564 40.153257, -79.913332 40....\n\n\n\n\n\n\n\nFor population with disability (as well as aging population, though not plotted), the spatial pattern is not that apparent. However, for racial minority population, it is definitely more concentrated in urban areas (Philadelphia and Pittsburgh). Meanwhile, some census tracts in northeastern and central Pennsylvania also exhibit more diversity in racial composition.",
    "crumbs": [
      "Methodology",
      "Machine Learning Data Preparation"
    ]
  },
  {
    "objectID": "analysis/PrepareML.html#land-cover-data",
    "href": "analysis/PrepareML.html#land-cover-data",
    "title": "Machine Learning Data Preparation",
    "section": "Land Cover Data",
    "text": "Land Cover Data\nLet’s load in all of our land cover data. There are six of them in total representing different land cover groups: forest, grasses, high density development, low density development, water and wetland. Below is an example of the forest land cover dataset that contains four columns. In addition to the GEOID and the file name identifier, it includes the total forest pixels and number of contiguous forest pixels in each census tract.\n\n\nCode\nlandcover[1].head(3)\n\n\n\n\n\n\n\n\n\ngeoid\nneighbor_landcover\ntotal_landcover\nsource_file\n\n\n\n\n0\n42001030101\n6885.043137\n14626.749020\nforest_landcover_metrics.csv\n\n\n1\n42001030103\n2053.384314\n5035.964706\nforest_landcover_metrics.csv\n\n\n2\n42001030104\n3582.129412\n10099.086275\nforest_landcover_metrics.csv\n\n\n\n\n\n\n\nThe code below demonstrates the process of merging multiple landcover datasets based on a common key (geoid), and then performing a series of transformations to organize and compute relevant landcover metrics. The code begins by merging the first dataset landcover[0] with the next dataset landcover[1] using a left join on the geoid column. This ensures that the resulting merged_df contains all rows from the first dataset, along with matching rows from the second dataset. The columns are then renamed to more meaningful names like forest_total, forest_neighbor, and grasses_total. Unnecessary are dropped to clean up the data. The process of merging, renaming, and dropping columns is repeated for several additional datasets (landcover[2] to landcover[5]), progressively adding landcover data related to wetlands, water, and development density.\n\n\nCode\n# combine landcover data\nmerged_df= pd.merge(landcover[0], landcover[1], on=\"geoid\", how=\"left\")\nmerged_df = merged_df.rename(\n    columns={\n        \"total_landcover\": \"forest_total\",\n        \"neighbor_landcover\": \"forest_neighbor\",\n        \"sum\": \"grasses_total\",\n    }\n)\nmerged_df = merged_df.drop(columns=[\"source_file_x\", \"source_file_y\"])\nmerged_df = pd.merge(merged_df, landcover[2], on=\"geoid\", how=\"left\")\nmerged_df = merged_df.rename(\n    columns={\n        \"sum\": \"wetland_total\",}\n)\nmerged_df = merged_df.drop(columns=[\"source_file\"])\nmerged_df = pd.merge(merged_df, landcover[3], on=\"geoid\", how=\"left\")\nmerged_df = merged_df.rename(\n    columns={\n        \"sum\": \"water_total\"    }\n)\nmerged_df = merged_df.drop(columns=[\"source_file\"])\nmerged_df = pd.merge(merged_df, landcover[4], on=\"geoid\", how=\"left\")\nmerged_df = merged_df.rename(\n    columns={\n        \"sum\": \"Ldensity_total\"    }\n)\nmerged_df = merged_df.drop(columns=[\"source_file\"])\nmerged_df = pd.merge(merged_df, landcover[5], on=\"geoid\", how=\"left\")\nmerged_df = merged_df.rename(\n    columns={\n        \"total_landcover\": \"Hdensity_total\",\n        \"neighbor_landcover\": \"Hdensity_neighbor\",    }\n)\nPA_Landcover = merged_df.drop(columns=[\"source_file\"])\n\n# some metrics with landcover data\nPA_Landcover['forest_pct'] = PA_Landcover['forest_neighbor'] / PA_Landcover['forest_total']\nPA_Landcover['Hdensity_pct'] = PA_Landcover['Hdensity_neighbor'] / PA_Landcover['Hdensity_total']\nPA_Landcover = PA_Landcover.drop(columns=[\"forest_neighbor\", \"Hdensity_neighbor\"])\n\n\nThe final dataset would look like the following:\n\n\nCode\nPA_Landcover.head(3)\n\n\n\n\n\n\n\n\n\ngeoid\ngrasses_total\nforest_total\nwetland_total\nwater_total\nLdensity_total\nHdensity_total\nforest_pct\nHdensity_pct\n\n\n\n\n0\n42001030101\n31291.925490\n14626.749020\n2675.717647\n288.745098\n5868.972549\n817.870588\n0.470716\n0.084107\n\n\n1\n42001030103\n11254.184314\n5035.964706\n408.607843\n879.866667\n3644.113725\n283.894118\n0.407744\n0.042269\n\n\n2\n42001030104\n40245.227451\n10099.086275\n2142.858824\n405.870588\n6041.019608\n1041.843137\n0.354698\n0.032345\n\n\n\n\n\n\n\nWe could also visualize the spatial distribution of the aggregated number of pixels for each kind of land cover in Pennsylvania. We may see that grasslands and forests are mainly concentrated in central and northern Pennsylvania. Welands are mainly concentrated in western Pennsylvania. Eastern Pennsylvania has more high density development than the western part.",
    "crumbs": [
      "Methodology",
      "Machine Learning Data Preparation"
    ]
  },
  {
    "objectID": "analysis/PrepareML.html#landsat-data",
    "href": "analysis/PrepareML.html#landsat-data",
    "title": "Machine Learning Data Preparation",
    "section": "Landsat Data",
    "text": "Landsat Data\nThe folder that stores Landsat data contains 16 files, with 4 vegetation indices for 4 seasons. Each of them is an individual file. Below is an example of one of the files. It only has three column, in which mean refers to the mean of savi in fall for each census tract.\n\n\nCode\nlandsat[0].head(3)\n\n\n\n\n\n\n\n\n\nGEOID\nmean\nsource_file\n\n\n\n\n0\n42001030101\n0.395006\nsavi_fall.csv\n\n\n1\n42001030103\n0.341787\nsavi_fall.csv\n\n\n2\n42001030104\n0.359570\nsavi_fall.csv\n\n\n\n\n\n\n\nWe need to perform the same operation as above to merge all 16 files together. The process starts by merging the first two datasets in the landsat list using a left join on GEOID, and it renames the relevant columns for clarity, specifically renaming the columns representing different vegetation indices (e.g., savi_fall, lst_summer). Unnecessary columns are then dropped to clean up the data. This merging and renaming process is repeated for additional datasets in the landsat list (from landsat[2] to landsat[15]), progressively adding columns related to different vegetation indices (e.g., evi_spring, lst_winter, ndvi_summer) for various seasons (spring, summer, fall, winter). Once all the datasets are merged, the final dataframe, PA_Landsat, contains a comprehensive set of vegetation index data for different seasons.\n\n\nCode\n# combine landsat data\nmerged_df2= pd.merge(landsat[0], landsat[1], on=\"GEOID\", how=\"left\")\nmerged_df2 = merged_df2.rename(\n    columns={\n        \"mean_x\": \"savi_fall\",\n        \"mean_y\": \"lst_summer\",\n    }\n)\nmerged_df2 = merged_df2.drop(columns=[\"source_file_x\", \"source_file_y\"])\nmerged_df2 = pd.merge(merged_df2, landsat[2], on=\"GEOID\", how=\"left\")\nmerged_df2 = merged_df2.rename(\n    columns={\n        \"mean\": \"evi_spring\",\n    }\n)\nmerged_df2 = merged_df2.drop(columns=[\"source_file\"])\nmerged_df2 = pd.merge(merged_df2, landsat[3], on=\"GEOID\", how=\"left\")\nmerged_df2 = merged_df2.rename(\n    columns={\n        \"mean\": \"evi_winter\",\n    }\n)\nmerged_df2 = merged_df2.drop(columns=[\"source_file\"])\nmerged_df2 = pd.merge(merged_df2, landsat[4], on=\"GEOID\", how=\"left\")\nmerged_df2 = merged_df2.rename(\n    columns={\n        \"mean\": \"evi_summer\",\n    }\n)\nmerged_df2 = merged_df2.drop(columns=[\"source_file\"])\nmerged_df2 = pd.merge(merged_df2, landsat[5], on=\"GEOID\", how=\"left\")\nmerged_df2 = merged_df2.rename(\n    columns={\n        \"mean\": \"lst_spring\",\n    }\n)\nmerged_df2 = merged_df2.drop(columns=[\"source_file\"])\nmerged_df2 = pd.merge(merged_df2, landsat[6], on=\"GEOID\", how=\"left\")\nmerged_df2 = merged_df2.rename(\n    columns={\n        \"mean\": \"evi_fall\",\n    }\n)\nmerged_df2 = merged_df2.drop(columns=[\"source_file\"])\nmerged_df2 = pd.merge(merged_df2, landsat[7], on=\"GEOID\", how=\"left\")\nmerged_df2 = merged_df2.rename(\n    columns={\n        \"mean\": \"lst_winter\",\n    }\n)\nmerged_df2 = merged_df2.drop(columns=[\"source_file\"])\nmerged_df2 = pd.merge(merged_df2, landsat[8], on=\"GEOID\", how=\"left\")\nmerged_df2 = merged_df2.rename(\n    columns={\n        \"mean\": \"ndvi_summer\",\n    }\n)\nmerged_df2 = merged_df2.drop(columns=[\"source_file\"])\nmerged_df2 = pd.merge(merged_df2, landsat[9], on=\"GEOID\", how=\"left\")\nmerged_df2 = merged_df2.rename(\n    columns={\n        \"mean\": \"savi_spring\",\n    }\n)\nmerged_df2 = merged_df2.drop(columns=[\"source_file\"])\nmerged_df2 = pd.merge(merged_df2, landsat[10], on=\"GEOID\", how=\"left\")\nmerged_df2 = merged_df2.rename(\n    columns={\n        \"mean\": \"lst_fall\",\n    }\n)\nmerged_df2 = merged_df2.drop(columns=[\"source_file\"])\nmerged_df2 = pd.merge(merged_df2, landsat[11], on=\"GEOID\", how=\"left\")\nmerged_df2 = merged_df2.rename(\n    columns={\n        \"mean\": \"savi_winter\",\n    }\n)\nmerged_df2 = merged_df2.drop(columns=[\"source_file\"])\nmerged_df2 = pd.merge(merged_df2, landsat[12], on=\"GEOID\", how=\"left\")\nmerged_df2 = merged_df2.rename(\n    columns={\n        \"mean\": \"savi_summer\",\n    }\n)\nmerged_df2 = merged_df2.drop(columns=[\"source_file\"])\nmerged_df2 = pd.merge(merged_df2, landsat[13], on=\"GEOID\", how=\"left\")\nmerged_df2 = merged_df2.rename(\n    columns={\n        \"mean\": \"ndvi_winter\",\n    }\n)\nmerged_df2 = merged_df2.drop(columns=[\"source_file\"])\nmerged_df2 = pd.merge(merged_df2, landsat[14], on=\"GEOID\", how=\"left\")\nmerged_df2 = merged_df2.rename(\n    columns={\n        \"mean\": \"ndvi_fall\",\n    }\n)\nmerged_df2 = merged_df2.drop(columns=[\"source_file\"])\nmerged_df2 = pd.merge(merged_df2, landsat[15], on=\"GEOID\", how=\"left\")\nmerged_df2 = merged_df2.rename(\n    columns={\n        \"mean\": \"ndvi_spring\",\n    }\n)\nPA_Landsat = merged_df2.drop(columns=[\"source_file\"])\n\n\nLet’s check the output of the merge.\n\n\nCode\nPA_Landsat.head(3)\n\n\n\n\n\n\n\n\n\nGEOID\nsavi_fall\nlst_summer\nevi_spring\nevi_winter\nevi_summer\nlst_spring\nevi_fall\nlst_winter\nndvi_summer\nsavi_spring\nlst_fall\nsavi_winter\nsavi_summer\nndvi_winter\nndvi_fall\nndvi_spring\n\n\n\n\n0\n42001030101\n0.395006\n33.257296\n0.319802\n0.260378\n0.568514\n10.827598\n0.410165\n4.714395\n0.762120\n0.316083\n19.379386\n0.263375\n0.517445\n0.473559\n0.672438\n0.501817\n\n\n1\n42001030103\n0.341787\n33.251842\n0.291822\n0.238932\n0.537638\n9.268735\n0.349453\n4.470871\n0.736301\n0.288043\n19.461976\n0.241329\n0.491634\n0.434709\n0.620192\n0.470976\n\n\n2\n42001030104\n0.359570\n33.716166\n0.337012\n0.280906\n0.563929\n10.559965\n0.368580\n4.283530\n0.753435\n0.328228\n19.039489\n0.279969\n0.512783\n0.468789\n0.616475\n0.509266\n\n\n\n\n\n\n\nWe could also visualize all the variables we have. Taking into the account the influence of seasonality is an important decision for more accurate prediction of COPD prevalence given that vegetation indices would change significant depending on the season should there be a correlation between them and COPD prevalence. In addition, spatial variations in vegetation indices across Pennsylvania is also evident in this map.",
    "crumbs": [
      "Methodology",
      "Machine Learning Data Preparation"
    ]
  },
  {
    "objectID": "analysis/PrepareML.html#merging-all-data",
    "href": "analysis/PrepareML.html#merging-all-data",
    "title": "Machine Learning Data Preparation",
    "section": "Merging All Data",
    "text": "Merging All Data\nThe very last step in preparing our dataframe for machine learning is to merge all dataframes together. The GEOID column in each dataset is explicitly converted to a string type to ensure consistency in the merging process. The datasets include information about chronic disease statistics from PA_Chronic, health risk behaviors from PA_HRB, retailer density from PA_Retailers, demographic factors from PA_Census, landcover from PA_Landcover, and satellite imagery data from PA_Landsat. The purpose of these merges is to create a comprehensive dataset, PA_Final, which combines relevant health, demographic, environmental, and spatial factors for each geographic area (defined by GEOID). This combined dataset will be useful for analyzing spatial relationships between chronic health conditions, environmental factors, and socio-economic characteristics, enabling more targeted health interventions and spatial analysis in Pennsylvania. Additionally, missing values in the final dataset are filled with zeros (fillna(0)),\n\n\nCode\n# everthing combined\nPA_Retailers['GEOID'] = PA_Retailers['GEOID'].astype(str)\nPA_Landsat['GEOID'] = PA_Landsat['GEOID'].astype(str)\nPA_Landcover['GEOID'] = PA_Landcover['geoid'].astype(str)\nPA_Census['GEOID'] = PA_Census['GEOID'].astype(str)\nPA_Chronic['GEOID'] = PA_Chronic['LocationName'].astype(str)\nPA_HRB['GEOID'] = PA_HRB['LocationName'].astype(str)\n\nPA_Final = pd.merge(PA_Chronic, PA_HRB[[\"GEOID\", \"Smoking\", \"Drinking\", \"Short_Sleep\", \"Physical_Activity\"]], on=\"GEOID\", how=\"inner\")\nPA_Final = pd.merge(PA_Final, PA_Retailers[[\"GEOID\", \"neighbor_avg_density\"]],  on=\"GEOID\", how=\"inner\")\nPA_Final = pd.merge(PA_Final, PA_Census[[\"GEOID\", \"minority\", \"aging\", \"disability\"]],  on=\"GEOID\", how=\"inner\")\nPA_Final = pd.merge(PA_Final, PA_Landcover.drop(columns=[\"geoid\"]),  on=\"GEOID\", how=\"inner\")\nPA_Final = pd.merge(PA_Final, PA_Landsat,  on=\"GEOID\", how=\"inner\")\nPA_Final = PA_Final.fillna(0)\n\n\nBelow is a snap shot of the final dataframe.\n\n\nCode\nPA_Final.head(5)\n\n\n\n\n\n\n\n\n\nCountyName\nLocationName\nAsthma\nTotalPopulation\nTotalPop18plus\nCOP\nGEOID\nSmoking\nDrinking\nShort_Sleep\n...\nevi_fall\nlst_winter\nndvi_summer\nsavi_spring\nlst_fall\nsavi_winter\nsavi_summer\nndvi_winter\nndvi_fall\nndvi_spring\n\n\n\n\n0\nAllegheny\n42003141200\n9.7\n4007\n3242\n4.8\n42003141200\n10.3\n21.3\n32.1\n...\n0.254667\n5.846844\n0.569421\n0.274674\n23.110598\n0.126746\n0.342207\n0.316698\n0.485495\n0.469827\n\n\n1\nAllegheny\n42003140100\n10.6\n5579\n5066\n4.2\n42003140100\n10.7\n23.9\n34.5\n...\n0.299216\n6.916926\n0.570255\n0.297535\n21.461009\n0.155348\n0.365760\n0.347601\n0.512010\n0.488617\n\n\n2\nAllegheny\n42003191900\n10.6\n2177\n1786\n5.9\n42003191900\n14.6\n22.1\n35.0\n...\n0.250319\n8.235525\n0.489839\n0.272597\n21.101579\n0.134626\n0.307345\n0.277480\n0.455361\n0.446655\n\n\n3\nAllegheny\n42003320700\n10.7\n1613\n1318\n7.3\n42003320700\n16.4\n21.3\n36.0\n...\n0.327833\n4.928171\n0.654718\n0.350588\n19.085399\n0.144300\n0.430196\n0.367408\n0.560819\n0.565722\n\n\n4\nAllegheny\n42003100500\n11.3\n2013\n1695\n7.7\n42003100500\n15.3\n15.8\n40.6\n...\n0.312338\n4.781316\n0.632465\n0.330390\n20.809972\n0.164339\n0.414667\n0.356294\n0.547767\n0.538620\n\n\n\n\n5 rows × 39 columns",
    "crumbs": [
      "Methodology",
      "Machine Learning Data Preparation"
    ]
  },
  {
    "objectID": "analysis/PrevalenceEst.html",
    "href": "analysis/PrevalenceEst.html",
    "title": "Seasonal Prevalence Estimation and Dimensionality Reduction",
    "section": "",
    "text": "This short Python notebook aims to estimate the seasonal prevalence of Chronic Obstructive Pulmonary Disease (COPD) and examine multicollinearity among predictor variables. Our current dataset calculates COPD on an annual basis; however, seasonal variations can significantly affect local environmental conditions, which may, in turn, influence COPD rates. To address this, we have acquired vegetation indices from different seasons, but at the same time, we also need an estimate of COPD rate for each season. We will split our dataframe into four separate dataframes—one for each season—and running machine learning models on these subsets. This approach avoids the potential errors introduced by averaging vegetation indices over an entire year. Additionally, dimensionality reduction is essential as many variables in our dataset may be highly correlated, leading to multicollinearity issues. For this analysis, we will only require the geopandas, pandas, and sklearn libraries.\nCode\nimport numpy as np\nimport pandas as pd\nimport geopandas as gpd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score,  mean_absolute_error",
    "crumbs": [
      "Methodology",
      "Seasonal Prevalence Estimation and Dimensionality Reduction"
    ]
  },
  {
    "objectID": "analysis/PrevalenceEst.html#seasonal-prevalence",
    "href": "analysis/PrevalenceEst.html#seasonal-prevalence",
    "title": "Seasonal Prevalence Estimation and Dimensionality Reduction",
    "section": "Seasonal Prevalence",
    "text": "Seasonal Prevalence\nThe approach we took here to estimate seasonal prevalence of COPD based on annual data is to look for exisiting literatures. In a study by So, Jennifer Y et al (2018), they analyzed data from 1,175 subjects without cardiovascular risk factors, and investigated seasonal and regional variability in COPD exacerbations across 45 study sites in North America. The results showed that winter had the highest COPD exacerbation rate (0.13 exacerbations/person-month), significantly greater than spring, summer, and fall (0.11, 0.079, and 0.10 exacerbations/person-month, respectively; P &lt; 0.001). This is not suprising result, as colder temperatures are known to increase hospitalization and mortality rates in adults with COPD and cardiac conditions, but the exacerbation rate they computed can be a helpful resource for us to compute a seasonal estimate. Their full article is avaialable here.\nRates of exacerbations in each season in each region\n Source: So, Jennifer Y et al (2018)\nBased on these information, the function calculate_seasonal_exacerbations first normalizes the input seasonal rates by dividing each rate by the total of all rates, ensuring they sum to 1. This normalization ensures that the rates are proportional and can be directly applied to the data. Next, for each season, the function calculates the number of seasonal exacerbations (season_count) for each row by multiplying the total COPD cases (total_column) by the normalized seasonal rate. It also computes the seasonal exacerbation rate as a percentage of the total population (season_rate).\n\ndef calculate_seasonal_exacerbations(df, total_column, seasonal_rates):\n\n    # Normalize the seasonal rates\n    total_rate = sum(seasonal_rates.values())\n    normalized_rates = {season: rate / total_rate for season, rate in seasonal_rates.items()}\n    \n    # Calculate seasonal exacerbations for each row in the DataFrame\n    for season, rate in normalized_rates.items():\n        df[f'{season}_count'] = df[total_column] * rate\n        df[f'{season}_rate'] = df[f'{season}_count'] / df['TotalPopulation'] * 100\n\n    return df\n\nThe seasonal rates are predefined as 0.39 for winter, 0.33 for spring, 0.237 for summer, and 0.30 for fall, based on observed data. Before calling the function, the total number of COPD-affected individuals is calculated for each row by converting the COPD rate from a percentage to an absolute count using the population column (TotalPopulation). This adjusted dataset is then processed by the function to estimate the seasonal COPD exacerbations for the population in Pennsylvania PA_Final.\n\nseasonal_rates = {\n    'winter': 0.39,  # Winter rate\n    'spring': 0.33,  # Spring rate\n    'summer': 0.237, # Summer rate\n    'fall': 0.30     # Fall rate\n}\n\nPA_Final[\"COP_People\"] = PA_Final[\"COP\"] / 100 * PA_Final[\"TotalPopulation\"]\nPA_Final = calculate_seasonal_exacerbations(PA_Final, 'COP_People', seasonal_rates)\n\nAfter all these, we proceed to create separate dataframe for each season. As an example, PA_Spring contains selected columns relevant to the analysis of COPD exacerbations during the spring season. This includes demographic, behavioral, and environmental variables such as Asthma, Smoking, forest_pct, lst_spring, and others, along with spring-specific exacerbation metrics (spring_count and spring_rate). These variables allow for targeted analysis of seasonal factors influencing COPD in spring. This process is repeated similarly for the other three seasons (winter, summer, and fall), with the corresponding seasonal columns (e.g., winter_count, lst_winter, etc.) included in their respective DataFrames. This separation enables season-specific analysis while maintaining consistency in the variables studied.\n\n\nCode\n# spring dataframe\nPA_Spring = PA_Final[['CountyName', 'Asthma', 'TotalPopulation', 'COP', 'GEOID', 'Smoking', \n                      'Drinking', 'Short_Sleep', 'Physical_Activity', 'neighbor_avg_density', \n                      'minority', 'aging', 'disability', 'forest_total', 'wetland_total', 'Hdensity_total',\n                      'Ldensity_total', 'grasses_total', 'water_total', 'forest_pct',  'Hdensity_pct', \n                      'evi_spring','lst_spring', 'ndvi_spring', 'savi_spring', 'spring_count', \"spring_rate\"]]",
    "crumbs": [
      "Methodology",
      "Seasonal Prevalence Estimation and Dimensionality Reduction"
    ]
  },
  {
    "objectID": "analysis/PrevalenceEst.html#dimensionality-reduction",
    "href": "analysis/PrevalenceEst.html#dimensionality-reduction",
    "title": "Seasonal Prevalence Estimation and Dimensionality Reduction",
    "section": "Dimensionality Reduction",
    "text": "Dimensionality Reduction\nDimensionality reduction is an important step before runing any machine learning models because many of the predictor variables are likely to be correlated with each other, which can lead to multicollinearity. Multicollinearity occurs when two or more predictors in a model are highly correlated, making it difficult to determine the individual effect of each predictor on the target variable. This can result in unstable coefficient estimates, increased variance, and reduced model interpretability.\nFor instance, environmental factors such as forest_total, wetland_total, and grasses_total may exhibit correlations, while demographic variables like minority, aging, and disability could also be related. Multicollinearity makes it difficult to isolate the effect of each individual predictor on the target variable, potentially destabilizing model estimates. Additionally, some variables like Hdensity_total and Ldensity_total, or the vegetation indices evi_spring, ndvi_spring, savi_spring, may be redundant and provide overlapping information, which increases the model’s complexity without adding predictive value. This redundancy and high correlation among predictors increase the risk of overfitting, where the model captures noise rather than true patterns in the data.\nIndeed, upon checking the correlation matrix for all of our variables, we found that there’s strong correlation between between vegetation indicies, different land cover features, and different health risks behaviors. Including all of them in our model will lead to overfitting.\n\n\n\n\n\n\n\n\n\n\nfeature_columns  = ['Smoking', 'Drinking', 'Short_Sleep', 'Physical_Activity', 'neighbor_avg_density', \n                    'minority', 'aging', 'disability', 'forest_total', 'wetland_total', 'Hdensity_total',\n                    'Ldensity_total', 'grasses_total', 'water_total', 'forest_pct',  'Hdensity_pct', \n                    'evi_spring','lst_spring', 'ndvi_spring', 'savi_spring']\ntarget_column = \"spring_rate\"\n\nX = PA_Spring[feature_columns]\ny = PA_Spring[target_column]\n\nNow, we will apply linear regression using the scikit-learn library to perform dimensionality reduction by calculating the Bayesian Information Criterion (BIC), using the spring dataset as an example. The BIC will help identify the most relevant predictor variables, mitigating the impact of multicollinearity and enhancing the interpretability of our machine learning models.\nThe Bayesian Information Criterion (BIC) is a model selection criterion that balances the goodness of fit with the complexity of the model. It is particularly useful for dimensionality reduction, as it helps penalize the inclusion of irrelevant predictor variables. The formula for BIC is:\n\\[\n\\text{BIC} = n \\cdot \\log(\\text{MSE}) + p \\cdot \\log(n)\n\\]\nWhere n is the number of samples, MSE is the mean squared error (a measure of model fit), and p is the number of predictors in the model.\nThe BIC penalizes models with more parameters to avoid overfitting, encouraging the selection of simpler models that still provide a good fit. The following code uses BIC to perform dimensionality reduction and eliminate irrelevant predictors from the model. The first step in the code is to fit the full model using all available predictors (X). The model is trained using LinearRegression() from scikit-learn, and the predictions (y_pred_full) are generated. The BIC for this full model is then calculated.\nNext, the code performs backward elimination to reduce the number of predictors. It starts with all predictors in the model and iteratively removes the least significant predictors based on BIC. - For each iteration, the BIC for models excluding one predictor at a time is calculated. - If removing a predictor results in a lower BIC (indicating a better model with fewer predictors), the predictor is excluded from the model. - This process continues until no further improvement in BIC is achieved by removing predictors.\n\ndef calculate_bic(y_true, y_pred, n, p):\n    mse = mean_squared_error(y_true, y_pred)\n    return n * np.log(mse) + p * np.log(n)\n\n# Step 1: Fit the full model\nfull_model = LinearRegression().fit(X, y)\ny_pred_full = full_model.predict(X)\nn_samples = len(y)\np_predictors = X.shape[1]\nbic_full = calculate_bic(y, y_pred_full, n_samples, p_predictors)\n\n# Step 2: Perform backward elimination\nselected_predictors = list(X.columns)\nselected_bic = bic_full\n\nwhile True:\n    best_bic = selected_bic\n    best_predictors = selected_predictors\n\n    for predictor in selected_predictors:\n        predictors_subset = selected_predictors.copy()\n        predictors_subset.remove(predictor)\n        X_subset = X[predictors_subset]\n        model = LinearRegression().fit(X_subset, y)\n        y_pred_subset = model.predict(X_subset)\n        bic = calculate_bic(y, y_pred_subset, n_samples, len(predictors_subset))\n        if bic &lt; best_bic:\n            best_bic = bic\n            best_predictors = predictors_subset\n\n    if best_bic &lt; selected_bic:\n        selected_bic = best_bic\n        selected_predictors = best_predictors\n    else:\n        break\n\n# Print the selected predictors and their BIC\nprint(\"Selected predictors:\", selected_predictors)\nprint(\"BIC:\", selected_bic)\n\nSelected predictors: ['Smoking', 'Drinking', 'Short_Sleep', 'Physical_Activity', 'minority', 'aging', 'disability', 'forest_total', 'Ldensity_total', 'grasses_total', 'forest_pct', 'lst_spring']\nBIC: -12292.878498899865\n\n\nThe backward elimination procedure identified the following selected predictors as those contributing most significantly to the model’s fit while minimizing the BIC:\n\nSmoking: The smoking rate in the population.\nDrinking: The alcohol consumption rate in the population.\nShort_Sleep: The proportion of the population reporting insufficient sleep.\nPhysical_Activity: The level of physical activity in the population.\nminority: The proportion of the population belonging to minority groups.\naging: The aging population within the dataset.\ndisability: The proportion of the population with disabilities.\nforest_total: Total forested area in the region.\nLdensity_total: Total land density (likely referring to residential or urban areas).\ngrasses_total: Total grassland area in the region.\nforest_pct: The percentage of the region covered by forests.\nlst_spring: Land Surface Temperature (LST) for the spring season.\n\nThese predictors have been retained because their inclusion resulted in the lowest Bayesian Information Criterion (BIC) value. The BIC for this model is -12292.88, indicating the best balance between goodness of fit and model complexity among the models tested.\nThis model is optimal in terms of fitting the data without overfitting, as indicated by the negative BIC value, which is lower compared to other possible models with more predictors.",
    "crumbs": [
      "Methodology",
      "Seasonal Prevalence Estimation and Dimensionality Reduction"
    ]
  },
  {
    "objectID": "analysis/VarCompare.html",
    "href": "analysis/VarCompare.html",
    "title": "Predictors Comparison Across Seasons",
    "section": "",
    "text": "In this notebook, we will evaluate the power of each of our predictors in predicting COPD and assess their effectiveness for different seasons. Specifically, we will run Random Forest Regressions on all of our predictors, predictors after dimensionality reduction, using only sociobehavioral variables, and using only environmental variables. While there are some multicollinearities in the dataset, we proceed with this approach as Random Forests handle multicollinearity by averaging over multiple trees, reducing the risk of overfitting and still providing valuable insights into feature importance.\nWe will mainly draw on the following packages from sklearn in this section:\nCode\nimport numpy as np\nimport pandas as pd\nimport geopandas as gpd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVR\nfrom sklearn.metrics import make_scorer, mean_squared_error, r2_score, mean_absolute_error\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import cross_validate, cross_val_predict\nimport tensorflow as tf\nfrom scipy.stats import norm\nimport warnings\nwarnings.filterwarnings('ignore')",
    "crumbs": [
      "Results",
      "Predictors Comparison Across Seasons"
    ]
  },
  {
    "objectID": "analysis/VarCompare.html#the-random-forest-model",
    "href": "analysis/VarCompare.html#the-random-forest-model",
    "title": "Predictors Comparison Across Seasons",
    "section": "The Random Forest Model",
    "text": "The Random Forest Model\nA Random Forest model is a versatile machine learning algorithm commonly used for regression analysis. It operates by constructing a multitude of decision trees during training and outputs the average of their predictions to provide robust and accurate results. Each tree in the forest is trained on a random subset of the data, with features also randomly sampled at each split, which helps reduce overfitting and ensures diversity among the trees. This ensemble approach improves model stability and accuracy by leveraging the collective predictions of multiple trees. Random Forest is particularly effective when dealing with datasets that have complex relationships, non-linear patterns, or a mix of categorical and continuous variables. It also provides insights into feature importance, allowing researchers to understand the relative contribution of each predictor in the model. Despite its strengths, Random Forest models can be computationally intensive and may require careful tuning of hyperparameters like the number of trees and maximum tree depth for optimal performance.\nRandom Forest Regressor\n\nSince we will be running the machine learning model multiple times for each seaon’s data, we defined some helper functions to streamline the process. The split_and_scale_data function prepares the data for Random Forest Regression by first splitting it into training and testing sets using train_test_split with stratification to maintain the target variable distribution. It then scales the features using StandardScaler to standardize the data, ensuring that all features contribute equally to the model. This step is crucial for consistent model performance, especially when features have varying scales (like ours).\n\n\nCode\ndef split_and_scale_data(X, y, stratify_col, test_size=0.2, random_state=42):\n   \n    # Split the data\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=test_size, random_state=random_state, stratify=stratify_col\n    )\n    \n    # Scale the data\n    scaler = StandardScaler()\n    X_train_scaled = scaler.fit_transform(X_train)\n    X_test_scaled = scaler.transform(X_test)\n    \n    return X_train_scaled, X_test_scaled, y_train, y_test, scaler\n\n\nThe train_and_evaluate_rf function trains a Random Forest model using the RandomForestRegressor with a specified number of estimators. It fits the model to the training data and then makes predictions on the test set. The function calculates key performance metrics, including Mean Squared Error (MSE), Root Mean Squared Error (RMSE), R-squared (R²), and Mean Absolute Error (MAE), to evaluate model accuracy. Additionally, it computes feature importances to identify which variables contribute most to the model’s predictions, returning the predictions, evaluation metrics, and a sorted dataframe of feature importances.\n\n\nCode\ndef train_and_evaluate_rf(X_train, X_test, y_train, y_test, feature_names, n_estimators=100, random_state=42):\n\n    # Train the Random Forest model\n    rf_model = RandomForestRegressor(n_estimators=n_estimators, random_state=random_state)\n    rf_model.fit(X_train, y_train)\n    \n    # Make predictions\n    predictions_rf = rf_model.predict(X_test)\n    \n    # Calculate metrics\n    mse_rf = mean_squared_error(y_test, predictions_rf)\n    rmse_rf = np.sqrt(mse_rf)\n    r2_rf = r2_score(y_test, predictions_rf)\n    mae_rf = mean_absolute_error(y_test, predictions_rf)\n\n    # Compute feature importances\n    feature_importances = rf_model.feature_importances_\n    importance_df = pd.DataFrame({\n        'Feature': feature_names,\n        'Importance': feature_importances\n    }).sort_values(by='Importance', ascending=False)\n    \n    return {\n        'predictions': predictions_rf,\n        'rmse': rmse_rf,\n        'r2': r2_rf,\n        'importance_df': importance_df\n    }\n\n\nThe extract_results function extracts key results from the output of a model evaluation. It retrieves the RMSE (Root Mean Squared Error) and R² (R-squared) metrics from the results, which provide insights into the model’s accuracy. The function also extracts the top 5 most important features based on their feature importances, which help in understanding which variables have the greatest influence on the model’s predictions. It returns both the performance metrics and the list of top features.\n\n\nCode\ndef extract_results(results, method_name):\n    \n    # Extract metrics\n    metrics = [results['rmse'], results['r2']]\n    \n    # Extract top 5 features\n    top_features = results['importance_df']['Feature'].head(5).tolist()\n    return metrics, top_features",
    "crumbs": [
      "Results",
      "Predictors Comparison Across Seasons"
    ]
  },
  {
    "objectID": "analysis/VarCompare.html#fitting-the-model",
    "href": "analysis/VarCompare.html#fitting-the-model",
    "title": "Predictors Comparison Across Seasons",
    "section": "Fitting the Model",
    "text": "Fitting the Model\nThe above functions will be applied separately using four different sets of predictors: all variables, only environmental variables, only sociobehavioral variables, and variables after dimensionality reduction. This approach allows us to assess the predictive power of each group of variables and determine their effectiveness in predicting COPD for each season. For each season, four models will be trained and evaluated.\n\n\nCode\n# using all variables \nstratify_col = PA_Spring['CountyName']\nX_Spring_All = PA_Spring[['Smoking', 'Drinking', 'Short_Sleep', 'Physical_Activity', 'neighbor_avg_density', \n                    'minority', 'aging', 'disability', 'forest_total', 'wetland_total', 'Hdensity_total',\n                    'Ldensity_total', 'grasses_total', 'water_total', 'forest_pct',  'Hdensity_pct', \n                    'evi_spring','lst_spring', 'ndvi_spring', 'savi_spring']]\ny_Spring = PA_Spring[['spring_count']]\nX_train_scaled, X_test_scaled, y_train, y_test_sp, scaler = split_and_scale_data(X_Spring_All, y_Spring, stratify_col)\nspring_result = train_and_evaluate_rf(X_train_scaled, X_test_scaled, y_train, y_test_sp, X_Spring_All.columns)                   \n\n\n\n\nCode\n# using all variables after dimensionality reduction\nX_Spring_BIC = PA_Spring[['Smoking', 'Drinking', 'Short_Sleep', 'Physical_Activity', 'minority', 'aging', 'disability', 'forest_total', 'Ldensity_total', 'grasses_total', 'forest_pct', 'lst_spring']]\nX_train_scaled, X_test_scaled, y_train, y_test_sp, scaler = split_and_scale_data(X_Spring_BIC, y_Spring, stratify_col)\nspring_result_BIC = train_and_evaluate_rf(X_train_scaled, X_test_scaled, y_train, y_test_sp, X_Spring_BIC.columns)\n\n\n\n\nCode\n# using all environmental predictors\nX_Spring_Env = PA_Spring[['forest_total', 'wetland_total', 'Hdensity_total',\n                    'Ldensity_total', 'grasses_total', 'water_total', 'forest_pct',  'Hdensity_pct', \n                    'evi_spring','lst_spring', 'ndvi_spring', 'savi_spring']]\nX_train_scaled, X_test_scaled, y_train, y_test_sp, scaler = split_and_scale_data(X_Spring_Env, y_Spring, stratify_col)\nspring_result_env = train_and_evaluate_rf(X_train_scaled, X_test_scaled, y_train, y_test_sp, X_Spring_Env.columns)\n\n\n\n\nCode\n# using all socio-behavioral predictors\nX_Spring_Sol = PA_Spring[['Smoking', 'Drinking', 'Short_Sleep', 'Physical_Activity', 'neighbor_avg_density', \n                      'minority', 'aging', 'disability']]\nX_train_scaled, X_test_scaled, y_train, y_test_sp, scaler = split_and_scale_data(X_Spring_Sol, y_Spring, stratify_col)\nspring_result_sol = train_and_evaluate_rf(X_train_scaled, X_test_scaled, y_train, y_test_sp, X_Spring_Sol.columns)",
    "crumbs": [
      "Results",
      "Predictors Comparison Across Seasons"
    ]
  },
  {
    "objectID": "analysis/VarCompare.html#goodness-of-fit-measure",
    "href": "analysis/VarCompare.html#goodness-of-fit-measure",
    "title": "Predictors Comparison Across Seasons",
    "section": "Goodness of Fit Measure",
    "text": "Goodness of Fit Measure\nAfter running the model 16 times (4 models for each of the 4 predictor sets across the seasons), we obtain 16 result outputs. From each model, we extract the R² and RMSE metrics as well as the top five predictors contributing the most to the predictions. The RMSE (Root Mean Squared Error) represents the average magnitude of prediction error, where lower values indicate better model performance. The R² (coefficient of determination) reflects the proportion of variance in the target variable that is explained by the predictors, with values closer to 1 indicating a better fit.\nAgain, the purpose of this analysis is to evaluate the predictive strength of different sets of variables and identify the most impactful predictors for COPD across seasons. All results are compiled into a unified table for easier interpretation, and the output is displayed below.\n\n\nCode\nmethods = ['sp_all', 'sp_bic', 'sp_env', 'sp_sol',\n           'su_all', 'su_bic', 'su_env', 'su_sol', \n           'fa_all', 'fa_bic', 'fa_env', 'fa_sol',\n           'wi_all', 'wi_bic', 'wi_env', 'wi_sol']  \nresults = [spring_result, spring_result_BIC, spring_result_env, spring_result_sol, \n           summer_result, summer_result_BIC, summer_result_env, summer_result_sol,\n           fall_result, fall_result_BIC, fall_result_env, fall_result_sol,\n           winter_result, winter_result_BIC, winter_result_env, winter_result_sol\n           ]\n\nmetrics_data = {}\ntop_features_data = {}\n\nfor method, result in zip(methods, results):\n    metrics, top_features = extract_results(result, method)\n    metrics_data[method] = metrics\n    top_features_data[method] = top_features\n\ncompare_metrics = pd.DataFrame(metrics_data, index=['RMSE', 'R2'])\ncompare_top_features = pd.DataFrame(top_features_data, index=[f'Feature {i+1}' for i in range(5)])\n\n\n\nComparing the R2 Across 16 Model\nTable Showing R2 Score By Model and Season\n\n\n\n\n\n\n\n\nModel\nall\nbic\nenv\nsol\n\n\nSeason\n\n\n\n\n\n\n\n\nFall\n0.561480\n0.504986\n0.385853\n0.269291\n\n\nSpring\n0.561549\n0.527482\n0.431229\n0.268616\n\n\nSummer\n0.560847\n0.500596\n0.386002\n0.269822\n\n\nWinter\n0.552514\n0.490316\n0.372439\n0.270203\n\n\n\n\n\n\n\nThe analysis of R² values reveals several important insights about the performance of our models in predicting COPD across different seasons and predictor subsets. First, models that include all predictor variables consistently achieve the highest R² scores for every season. This indicates that combining environmental and sociobehavioral factors provides the most comprehensive understanding of the variability in COPD, as these variables likely capture a wide range of relevant influences.\nWhen dimensionality reduction is applied, the R² values decrease, suggesting some loss of information. However, even with dimensionality reduction, the models explain more than half of the variability in COPD outcomes. This implies that while reducing the number of predictors sacrifices some predictive power, the retained variables are still meaningful contributors to the model. Comparing the models that use only environmental or sociobehavioral variables, the R² values are higher for the environmental variable models across all seasons.\nSeasonal differences emerge as a key aspect of the analysis. The spring models achieve the highest R² values when all predictors are used and when only environmental variables are included, but they exhibit the lowest R² when using only sociobehavioral variables. This trend is followed by the summer and fall models, which also show relatively strong R² values under similar conditions. In contrast, the winter models have the highest R² values when using only sociobehavioral variables, indicating a distinct seasonal dynamic.\nGiven that the primary difference between seasonal models lies in the local environmental vegetation indices, these results suggest that winter environmental factors may contribute less variability to COPD outcomes compared to sociobehavioral variables during this season. This could reflect reduced vegetation activity and lower pollen levels in winter, leading to less influence of environmental predictors. Conversely, the higher R² values for spring models with environmental predictors likely reflect the strong impact of vegetation indices and related environmental conditions, such as increased pollen levels and changing air quality, which are more pronounced during spring compared to winter\n\n\n\n\n\n\n\n\n\n\n\nComparing the RMSE Across 16 Models\nTable Showing RMSE By Model and Season\n\n\n\n\n\n\n\n\nModel\nall\nbic\nenv\nsol\n\n\nSeason\n\n\n\n\n\n\n\n\nFall\n20.206061\n21.468205\n23.912397\n26.083081\n\n\nSpring\n22.224898\n23.072179\n25.313270\n28.704642\n\n\nSummer\n15.974291\n17.034905\n18.888489\n20.598157\n\n\nWinter\n26.535056\n28.319169\n31.423747\n33.886842\n\n\n\n\n\n\n\nExamining the RMSE results, the summer models consistently exhibit the lowest RMSE across all model types, while the winter models show the highest RMSE. Including all variables results in significantly lower RMSE values, reflecting better predictive accuracy. Conversely, dimensionality reduction slightly increases RMSE, and the highest RMSE values are observed when using only sociobehavioral variables.\nThese RMSE findings align with the R² results. Lower RMSE and higher R² values in the “all predictors” models indicate that the inclusion of both environmental and sociobehavioral variables leads to more accurate and explanatory models for predicting COPD. The slightly higher RMSE and lower R² values after dimensionality reduction suggest some loss of information, which reduces the model’s predictive power but still maintains reasonable performance.\nSeasonally, the lowest RMSE in summer models suggest that summer conditions may lead to more predictable COPD outcomes, potentially due to less variability in environmental factors like vegetation indices or air quality during this time. On the other hand, the higher RMSE in winter, coupled with relatively lower R² values for environmental models, points to a weaker relationship between winter environmental conditions and COPD. This reinforces the earlier observation that sociobehavioral variables play a more prominent role in winter, where environmental variability is less influential.\n\n\n\n\n\n\n\n\n\n\n\nCompare Predictors with Highest Contribution\nIn models that include all predictor variables, development density stands out as the most significant contributor, followed closely by physical activity and neighborhood tobacco retail density. Smoking behaviors also play a role but come after the previous factors. These results indicate that population and urban infrastructure characteristics—such as density—along with lifestyle factors like physical activity and smoking, are crucial in explaining the variability in the models. This aligns with the notion that urban environments and behavioral factors are closely tied to the outcomes being modeled, such as health-related outcomes.\n\n\n\n\n\n\n\n\n\nsp_all\nsu_all\nfa_all\nwi_all\n\n\n\n\nFeature 1\nHdensity_total\nHdensity_total\nHdensity_total\nHdensity_total\n\n\nFeature 2\nLdensity_total\nLdensity_total\nLdensity_total\nLdensity_total\n\n\nFeature 3\nPhysical_Activity\nPhysical_Activity\nPhysical_Activity\nPhysical_Activity\n\n\nFeature 4\nneighbor_avg_density\nneighbor_avg_density\nneighbor_avg_density\nneighbor_avg_density\n\n\nFeature 5\nSmoking\nSmoking\nSmoking\nSmoking\n\n\n\n\n\n\n\nWhen we perform dimensionality reduction, the results shift slightly, but core contributors remain consistent. Density and physical activity continue to be among the most significant predictors, underscoring their consistent importance across various model configurations. However, other predictors such as land surface temperature (LST) by season and smoking behavior start to show a more prominent role. This shift indicates that once we reduce the number of variables, the remaining predictors capture more specific, localized aspects like seasonal temperature changes and personal health behaviors (e.g., smoking), which become increasingly important.\n\n\n\n\n\n\n\n\n\nsp_bic\nsu_bic\nfa_bic\nwi_bic\n\n\n\n\nFeature 1\nLdensity_total\nLdensity_total\nLdensity_total\nLdensity_total\n\n\nFeature 2\nPhysical_Activity\nPhysical_Activity\nPhysical_Activity\nPhysical_Activity\n\n\nFeature 3\nlst_spring\nlst_summer\nlst_fall\nforest_total\n\n\nFeature 4\nforest_total\nSmoking\nSmoking\nSmoking\n\n\nFeature 5\nSmoking\nminority\nforest_total\nminority\n\n\n\n\n\n\n\nIn models that only include social behavioral variables, it is not surprising to see that physical activity emerges as the top contributor.\n\n\n\n\n\n\n\n\n\nsp_sol\nsu_sol\nfa_sol\nwi_sol\n\n\n\n\nFeature 1\nPhysical_Activity\nPhysical_Activity\nPhysical_Activity\nPhysical_Activity\n\n\nFeature 2\nneighbor_avg_density\nneighbor_avg_density\nneighbor_avg_density\nneighbor_avg_density\n\n\nFeature 3\ndisability\ndisability\ndisability\ndisability\n\n\nFeature 4\naging\naging\naging\naging\n\n\nFeature 5\nSmoking\nSmoking\nSmoking\nSmoking\n\n\n\n\n\n\n\nTurning to environmental predictors, we observe that development density, surface temperature, and NDVI (Normalized Difference Vegetation Index) emerge as the most important. These factors are indicative of the environmental characteristics that influence health outcomes. However, seasonal differences emerge when we look specifically at the winter models. In winter, both vegetation indices and temperature seem to have less influence compared to the other seasons. This may be due to reduced vegetation activity and temperature fluctuations in winter, making these factors less relevant to the outcome in colder months.\n\n\n\n\n\n\n\n\n\nsp_env\nsu_env\nfa_env\nwi_env\n\n\n\n\nFeature 1\nHdensity_total\nHdensity_total\nHdensity_total\nHdensity_total\n\n\nFeature 2\nLdensity_total\nLdensity_total\nLdensity_total\nLdensity_total\n\n\nFeature 3\nlst_spring\nndvi_summer\nHdensity_pct\nHdensity_pct\n\n\nFeature 4\nHdensity_pct\nlst_summer\nlst_fall\nlst_winter\n\n\nFeature 5\nndvi_spring\nHdensity_pct\nndvi_fall\nndvi_winter\n\n\n\n\n\n\n\nBy examining these results together, we see how different types of predictors—social behavioral versus environmental—interact with seasonal changes and dimensionality reduction. The consistency of physical activity and development density top predictors across models highlights their importance as a fundamental health-related factor, irrespective of seasonal variations or environmental conditions. Conversely, environmental factors such as temperature and vegetation indices appear to be more sensitive to seasonal changes, with winter showing a distinct reduction in their relevance.",
    "crumbs": [
      "Results",
      "Predictors Comparison Across Seasons"
    ]
  },
  {
    "objectID": "analysis/VarCompare.html#understanding-prediction-error",
    "href": "analysis/VarCompare.html#understanding-prediction-error",
    "title": "Predictors Comparison Across Seasons",
    "section": "Understanding Prediction Error",
    "text": "Understanding Prediction Error\nIn the previous sections, we highlighted the importance of including both environmental and social behavioral predictors in our model for a comprehensive understanding of COPD. We also explored how seasonal differences influence the model’s performance. However, it is crucial to validate the effectiveness of these predictors in accurately measuring COPD.\nIn this section, we run random forest regression for each season, using the predictors after dimensionality reduction, to compare predicted and actual COPD rates at the census tract level. We will also create maps to visualize areas where the model under- or over-predicts, at both the census tract and county levels, as public health interventions are often implemented at the county level.\n\nPredicted VS. Actual\nIn this code, we are performing cross-validation (CV) to evaluate the performance of a random forest regression model, rather than using an explicit train-test split. Cross-validation is a technique that divides the dataset into multiple subsets (or “folds”) and iteratively trains the model on a portion of the data while testing it on the remaining data. This process is repeated for each fold, allowing the model to be validated on different portions of the data. As a result, cross-validation provides a more robust estimate of the model’s generalization performance because it reduces the risk of overfitting that might occur with a single train-test split.\n\n\nCode\ndef train_and_evaluate_rf_with_cv_predictions(X, y, n_estimators=100, random_state=42, cv=5):\n\n    # Define the model\n    rf_model = RandomForestRegressor(n_estimators=n_estimators, random_state=random_state)\n\n    # Define custom scorers\n    scoring = {\n        'mse': make_scorer(mean_squared_error, greater_is_better=False),\n        'r2': make_scorer(r2_score)\n    }\n\n    # Perform cross-validation to calculate metrics\n    cv_results = cross_validate(rf_model, X, y, scoring=scoring, cv=cv, return_train_score=False)\n\n    # Perform cross-validation to get predictions\n    predictions = cross_val_predict(rf_model, X, y, cv=cv)\n\n    results = {\n        'mean_rmse': np.sqrt(-np.mean(cv_results['test_mse'])),  # Convert negative MSE to positive RMSE\n        'mean_r2': np.mean(cv_results['test_r2']),\n        'predictions': predictions  # Cross-validated predictions for the entire dataset\n    }\n    \n    return results\n\n\nWhen we examine the residuals versus predicted plot, we may observe that the R² value has dropped compared to when we used a simple train-test split for the regression model. This is a natural outcome of using cross-validation, as the model is evaluated on multiple subsets of the data, which provides a more reliable estimate of its performance. Cross-validation tends to give a more conservative estimate of model accuracy because the model is tested on more diverse data points, making the evaluation less optimistic than a single train-test split might suggest.\nAcross the different seasons, we see that the variability in R² is relatively small, but there are still differences—specifically, the spring model tends to have the highest R², while the winter model has the lowest. This trend aligns with previous observations, reinforcing the notion that seasonal variations play a role in how well the model can predict COPD rates. The lower R² in winter suggests that the model may struggle more to capture patterns in the winter data, possibly due to seasonal factors like temperature or air quality that affect COPD differently across seasons.\nA key observation in the residual plot is the presence of a positive linear pattern. This pattern indicates that the model may be consistently underpredicting or overpredicting certain ranges of the data. In a well-fitted model, residuals should ideally be randomly distributed around zero, with no discernible pattern. The positive linear relationship suggests that as the predicted values increase, the residuals (or errors) also increase, meaning the model might not be capturing a certain aspect of the data adequately. This could point to a need for further feature engineering or model adjustments, such as incorporating additional variables or using a different algorithm, to better model the data and improve accuracy.\n\n\nCode\nseasons_data = {\n    \"Spring\": (X_Spring_BIC, y_Spring),\n    \"Summer\": (X_Summer_BIC, y_Summer),\n    \"Fall\": (X_Fall_BIC, y_Fall),\n    \"Winter\": (X_Winter_BIC, y_Winter)\n}\n\nseason_results = {}\n\nfor season, (X, y) in seasons_data.items():\n    cv_results = train_and_evaluate_rf_with_cv_predictions(X, y)\n    season_results[season] = {\n        'predictions': np.array(cv_results['predictions']),\n        'actual': y.values.flatten(), \n        'mean_rmse': cv_results['mean_rmse'],\n        'mean_r2': cv_results['mean_r2']\n    }\n\n\n\n\n\n\n\n\n\n\n\n\n\nSpatial Distribution of Errors\nWe also mapped the residuals at the census tract level to better understand where our model is underpredicting and overpredicting. In this context, positive residuals indicate areas where the model is underpredicting COPD rates, and negative residuals suggest overprediction. Upon initial inspection, the pattern is not immediately clear across all seasons, suggesting that more information is needed to understand the underlying causes of these prediction errors within different regions of Pennsylvania.\nHowever, a closer examination reveals more nuanced insights, particularly in urban areas like Philadelphia. In northern Philadelphia, the model tends to underpredict COPD rates, while in the surrounding suburban areas, there is more overprediction. Interestingly, very rural areas also show underprediction, which could reflect the lack of sufficient data or features that explain COPD patterns in these regions. This pattern is important because it points to geographic and demographic factors that the model may not be fully capturing.\nAnother finding is that if a census tract is underpredicting or overpredicting, nearby tracts tend to exhibit similar prediction errors. This suggests a spatial continuity in the model’s inaccuracies, possibly driven by regional characteristics that influence multiple neighboring tracts in the same way.\n\n\n\n\n\n\n\n\n\nWhen we average the residuals by county, the overall patterns shift significantly. Across all seasons, the model tends to underpredict COPD rates at the county level, with the exception of a few counties in northern Pennsylvania, where the model slightly overpredicts. This observation highlights how the geographic scale of analysis can influence our understanding of model performance and error distribution, thereby affecting policymaking and resource allocation strategies.\nAt the county level, this consistent underprediction suggests that broader, regional factors might be driving COPD rates in ways that the model does not fully capture. However, when we examine the residuals more closely, it becomes evident that the errors are not particularly large, even in areas with underprediction. This implies that while the model’s performance may vary by geography, its overall accuracy remains relatively stable.",
    "crumbs": [
      "Results",
      "Predictors Comparison Across Seasons"
    ]
  },
  {
    "objectID": "analysis/VectorData.html",
    "href": "analysis/VectorData.html",
    "title": "Vector Data and US Census Bureau API",
    "section": "",
    "text": "To get all the vector data required in this study, we used the following packages. The cenpy package provides a streamlined interface for accessing data from the U.S. Census Bureau’s APIs, such as the American Community Survey (ACS) and Decennial Census. It simplifies querying demographic, socioeconomic, and housing data directly from census databases. The pygris package focuses on working with the U.S. Census Bureau’s geographic data, particularly boundary files like TIGER/Line shapefiles. We would also need geopandas and pandas for working with spatial and tabular data, respectively, to load and further process downloaded data.\nNote: The dataset from CDC is 760 MB and therefore cannot be commited to GitHub. It is currently stored in a private folder ignored by the repository. Given the size of the original data, we used Colab for all the vector data loading and processing work as well. The code here are just for demonstration purposes. All file paths have been removed.\nIf you wish to reproduce this study, you may download the original CDC data that’s available here. Alternatively, you may head to the next step in the Methodology section and follow the steps there to load in proccessed data.",
    "crumbs": [
      "Methodology",
      "Vector Data and US Census Bureau API"
    ]
  },
  {
    "objectID": "analysis/VectorData.html#tobacco-retailer-data",
    "href": "analysis/VectorData.html#tobacco-retailer-data",
    "title": "Vector Data and US Census Bureau API",
    "section": "Tobacco Retailer Data",
    "text": "Tobacco Retailer Data\nThe original retailer dataset from PA Open Data Portal is first loaded from a CSV file. Next, the data is filtered to include only rows where the state column equals ‘PA’. This extra steps ensures that we are only including the retailers from Pennsylvania. The resulting subset is refined to retain only the columns of interest: county, license_type, lat, and lon, which provide relevant spatial and categorical information for further analysis.\n\nall_retailers = pd.read_csv('######')\npa_retailers = all_retailers[all_retailers['state'] == 'PA']\npa_retailers = pa_retailers[[\"county\", \"license_type\", \"lat\", \"lon\"]]\n\nThe processed data is then exported.\n\npa_retailers.to_csv('#####', index=False)",
    "crumbs": [
      "Methodology",
      "Vector Data and US Census Bureau API"
    ]
  },
  {
    "objectID": "analysis/VectorData.html#copd-data-and-health-risks-behaviors",
    "href": "analysis/VectorData.html#copd-data-and-health-risks-behaviors",
    "title": "Vector Data and US Census Bureau API",
    "section": "COPD Data and Health Risks Behaviors",
    "text": "COPD Data and Health Risks Behaviors\nAs mentioned in the introduction, the CDC data contains census tract level estimates of 27 disease measures for 500 major cities in the U.S, which totals about 3 million rows. After reading in the data, we need to significnatly trim down the dataset to the information we need: we are mainly interested in the prevalence of COPD as well as the number of adults with health risks behavior.\n\ncdc_data = pd.read_csv(\"######\")\n\nAccording to CDC, they computed a probability among adults who report having ever been told by a doctor, nurse, or other health professional they had chronic obstructive pulmonary disease (COPD), emphysema, or chronic bronchitis. The probability was then applied to the detailed population estimates at the appropriate geographic level to generate the prevalence. More information can be found here.\nWith the raw cdc_data, we select rows where Measure is either “Current asthma among adults” or “Chronic obstructive pulmonary disease among adults,” and the state abbreviation is “PA”. The asthma and COPD subsets are then merged based on the common column LocationName (essentially the GEOID), using a left join to preserve all locations in the asthma data. The merged dataset is renamed to include clearer column labels, where Asthma represents asthma prevalence and COP represents COPD prevalence.\n\n# process CRD data\nPA_Asthma = cdc_data[(cdc_data['Measure'] == \"Current asthma among adults\") & (cdc_data['StateAbbr'] == \"PA\")]\nPA_COP = cdc_data[(cdc_data['Measure'] == \"Chronic obstructive pulmonary disease among adults\") & (cdc_data['StateAbbr'] == \"PA\")]\nPA_Chronic = PA_Asthma.merge(\n    PA_COP[['LocationName', 'Data_Value']],\n    on=\"LocationName\",\n    how=\"left\"\n).rename(columns={\"Data_Value_x\": \"Asthma\", \"Data_Value_y\": \"COP\"})\n\nThis dataset is then exported to a new CSV file to hold our outcome variables. While we only intend to use COPD is the outcome variable, we also include asthma here in case we need it for further analysis.\n\nPA_Chronic.to_csv('######', index=False)\n\nWe then extract health risk behavior data from the same cdc_data. The PLACES Health Risk Behaviors data capture estimated prevalence for various U.S. adult behaviors that pose a risk to health, from binge drinking to smoking, lack of physical inactivity, and short sleep duration. Among those, binge drinking adults are those who report having ≥5 drinks (men) or ≥4 drinks (women) on ≥1 occasion during the previous 30 days. Smoking adults are those who report having smoked ≥ 100 cigarettes in their lifetime and currently smoke every day or some days. Lack of physical activity refers to adults having no leisure-time physical activity during the past month. Short sleep duration refers to those who have less than 7 hours of sleep.\nFor more detailed explaination of each variable, you may refer to this page.\nThe code below extracts subsets for four specific health behaviors. Then, each subset is filtered for rows where the Measure matches the behavior and the StateAbbr is “PA”. The subsets are sequentially merged on the common LocationName column using left joins, ensuring that all locations from the smoking data are preserved. The merged dataset, PA_HRB, contains columns for the prevalence of each behavior, renamed for clarity.\n\n# process HRB data\nPA_Smoking = cdc_data[(cdc_data['Measure'] == \"Current cigarette smoking among adults\") & (cdc_data['StateAbbr'] == \"PA\")]\nPA_Drinking = cdc_data[(cdc_data['Measure'] == \"Binge drinking among adults\") & (cdc_data['StateAbbr'] == \"PA\")]\nPA_Physical_Activity = cdc_data[(cdc_data['Measure'] == \"No leisure-time physical activity among adults\") & (cdc_data['StateAbbr'] == \"PA\")]\nPA_Short_Sleep = cdc_data[(cdc_data['Measure'] == \"Short sleep duration among adults\") & (cdc_data['StateAbbr'] == \"PA\")]\n\nPA_HRB = PA_Smoking.merge(\n    PA_Drinking[['LocationName', 'Data_Value']], on='LocationName', how='left'\n).rename(columns={\"Data_Value_x\": \"Smoking\", \"Data_Value_y\": \"Drinking\"})\n\nPA_HRB = PA_HRB.merge(\n    PA_Physical_Activity[['LocationName', 'Data_Value']], on='LocationName', how='left'\n).rename(columns={'Data_Value': 'Physical_Activity'})\n\nPA_HRB = PA_HRB.merge(\n    PA_Short_Sleep[['LocationName', 'Data_Value']], on='LocationName', how='left'\n).rename(columns={'Data_Value': 'Short_Sleep'})\nPA_HRB[['LocationName', 'Smoking', 'Drinking', 'Physical_Activity', 'Short_Sleep']]\n\nThis dataset is then exported to a new CSV file.\n\nPA_HRB.to_csv('######', index=False)",
    "crumbs": [
      "Methodology",
      "Vector Data and US Census Bureau API"
    ]
  },
  {
    "objectID": "analysis/VectorData.html#census-data",
    "href": "analysis/VectorData.html#census-data",
    "title": "Vector Data and US Census Bureau API",
    "section": "Census Data",
    "text": "Census Data\nThe code below connects to the Census Bureau’s ACS (American Community Survey) 5-Year Data (2022) using the cenpy package and specifies a set of variables to retrieve demographic, age-related, and disability-related characteristics. These include total population, race and ethnicity breakdowns (White, Black, Native American, Asian, and Hispanic populations), age distributions for individuals 65 years and older, and disability prevalence across various age and gender groups. By accessing these data, the project aims to understand the factors associated with public health outcomes, particularly COPD (Chronic Obstructive Pulmonary Disease).\n\nacs = cenpy.remote.APIConnection(\"ACSDT5Y2022\")\n\nIncluding these census variables is critical for analyzing COPD rates because the condition is influenced by demographic, social, and environmental factors. For example, COPD prevalence varies across racial and ethnic groups due to disparities in healthcare access, environmental exposures, and socioeconomic conditions, making it essential to include race and ethnicity data. Age distribution data is also vital since COPD predominantly affects older adults; understanding the size and characteristics of elderly populations helps identify at-risk groups. Additionally, disability prevalence provides insight into potential comorbidities and healthcare needs among populations with COPD, as individuals with disabilities often face heightened vulnerability to chronic conditions. By integrating these data, the project can explore correlations between COPD rates and demographic characteristics, identify vulnerable subpopulations, and inform targeted public health strategies. This comprehensive approach supports evidence-based decision-making to address health disparities and improve outcomes.\n\ncensus_var = [\"NAME\",\n              \"B02001_001E\", # total\n              \"B02001_002E\", # white\n              \"B02001_003E\", # black\n              \"B02001_004E\", # native american\n              \"B02001_005E\", # asian\n              \"B03002_012E\", # hispanic\n              'B01001_020E', # male 65-66\n              'B01001_021E', # male 67-69\n              'B01001_022E', # male 70-74\n              'B01001_023E', # male 75-79\n              'B01001_024E', # male 80-84\n              'B01001_025E', # male over 85\n              'B01001_044E', # female 65-66\n              'B01001_045E', # female 67-69\n              'B01001_046E', # female 70-74\n              'B01001_047E', # female 75-79\n              'B01001_048E', # female 80-84\n              'B01001_049E', # female over 85\n              'B18101_007E', # Male 5 to 17 years With a disability\n              'B18101_010E', # Male 18 to 34 years With a disability\n              'B18101_013E', # Male 35 to 64 years With a disability\n              'B18101_016E', # Male 65 to 74 years With a disability\n              'B18101_019E', # Male over 75 years With a disability\n              'B18101_026E', # Female 5 to 17 years With a disability\n              'B18101_029E', # Female 18 to 34 years With a disability\n              'B18101_032E', # Female 35 to 64 years With a disability\n              'B18101_035E', # Female 65 to 74 years With a disability\n              'B18101_038E'\n             ]\n\nAfter getting the data, we calculated the three indices we are interested in: percentage of racial minority population, percentage of population with disability, and percentage of aging population. Specifically, the minority column calculates the proportion of the population that identifies as a racial or ethnic minority. This is achieved by subtracting the White population B02001_002E from the total population B02001_001E and dividing by the total population. The aging column computes the proportion of elderly individuals aged 65 and older, using a sum of relevant age-specific population variables B01001_020E to B01001_049E divided by the total population. Similarly, the disability column calculates the proportion of the population with a disability, based on age and gender-specific disability data B18101_007E to B18101_038E.\n\npa_state_code = \"42\"\ncensus_data = acs.query(\n    cols=census_var,\n    geo_unit=\"tract\",\n    geo_filter={\"state\": pa_state_code}\n)\nfor variable in census_var:\n    if variable != \"NAME\":\n        census_data[variable] = census_data[variable].astype(float)\n\n\ncensus_data['minority'] = (\n    (census_data['B02001_001E'] - census_data['B02001_002E']) / census_data['B02001_001E']\n)\ncensus_data['aging'] = (\n    census_data[[\n        'B01001_020E', 'B01001_021E', 'B01001_022E', 'B01001_023E',\n        'B01001_024E', 'B01001_025E', 'B01001_044E', 'B01001_045E',\n        'B01001_046E', 'B01001_047E', 'B01001_048E', 'B01001_049E'\n    ]].sum(axis=1) / census_data['B02001_001E']\n)\ncensus_data['disability'] = (\n    census_data[[\n        'B18101_007E', 'B18101_010E', 'B18101_013E', 'B18101_016E',\n        'B18101_019E', 'B18101_026E', 'B18101_029E', 'B18101_032E',\n        'B18101_035E', 'B18101_038E'\n    ]].sum(axis=1) / census_data['B02001_001E']\n)\n\nThe last step here simply merges demographic metrics with geographic boundaries at the tract level. Key variables, including minority representation, aging population, and disability prevalence, are filtered alongside geographic identifiers. The purpose is to append GEOID onto the dataframe, making it easier to work with later.\n\ncensus_data = census_data[[\"NAME\", \"county\", \"tract\", \"minority\", \"aging\", \"disability\"]]\ntracts = pygris.tracts(state=pa_state_code, year=2022)\npa_census_data = tracts.merge(census_data, left_on=[\"COUNTYFP\", \"TRACTCE\"], right_on=[\"county\", \"tract\"],)\npa_census_data = pa_census_data[[\"GEOID\", \"minority\", \"aging\", \"disability\", \"geometry\"]]\n\n\npa_census_data.to_csv('######', index=False)",
    "crumbs": [
      "Methodology",
      "Vector Data and US Census Bureau API"
    ]
  },
  {
    "objectID": "analysis/RasterData.html",
    "href": "analysis/RasterData.html",
    "title": "Raster Data and Google Earth Engine",
    "section": "",
    "text": "To get remote sensing data, we used the following packages. The ee package (Earth Engine Python API) facilitates interaction with Google Earth Engine for large-scale geospatial processing. geemap supports visualization and analysis of Earth Engine data in Python. os is used for interacting with the file system, and requests handles HTTP requests to retrieve data from web sources. The datetime module manages date and time operations, while geopandas and pandas are essential for working with spatial and tabular data, respectively, to load and further process downloaded data.\nNote: Since the raster data used in this project are relatively large in size, all operations were conducted on Google Colab, which provides a more seamless environment for interacting with Google Earth Engine and is better at handling the processing of large datasets. The code here are just for demonstration purposes. All file paths have been removed.\nIf you wish to reproduce this study, this project requires authentication with Google Earth Engine (GEE) to access its geospatial data and computational capabilities. To begin, users must authenticate using ee.Authenticate() and initialize the connection to their GEE project with ee.Initialize(project=\"your_project_name\"). Ensure you have a valid Google Earth Engine account and have created a project within your GEE console. These steps enable the seamless integration of Earth Engine operations within the Python environment.\nAdditionally, users need to establish a connection with your Google Drive to store and manage data files. This is done using the following commands: from google.colab import drive then drive.mount('/content/drive')",
    "crumbs": [
      "Methodology",
      "Raster Data and Google Earth Engine"
    ]
  },
  {
    "objectID": "analysis/RasterData.html#study-area-preparation",
    "href": "analysis/RasterData.html#study-area-preparation",
    "title": "Raster Data and Google Earth Engine",
    "section": "Study Area Preparation",
    "text": "Study Area Preparation\nTo limit our study area to the state of Pennsylvania, we retrieved the geojson file of all Pennsylvania census tracts from the Open Data Pennsylvania. Next, the code dissolves the geometries of all features in the GeoDataFrame pa_tracts into a single geometry representing the boundary of Pennsylvania. Then, it extracts the exterior coordinates of the dissolved boundary geometry and converts them into a list format suitable for google earth engine. This list of coordinates is used to create a polygon geometry pa_geom using the ee.Geometry.Polygon() function from the Earth Engine Python API.\nThe polygon geometry representing the boundary of Pennsylvania is converted into an Earth Engine FeatureCollection (aoi). This FeatureCollection serves as the study area extent for subsequent analysis within Google Earth Engine and is used later in the study to clip the satellite images.\n\npa_tracts = gpd.read_file('######')\n# dissolve geometry to get the boundary\npa_bound = pa_tracts.dissolve() \n# convert the geometry into a format suitable for gee\npa_geom= ee.Geometry.Polygon(list(pa_bound['geometry'].iloc[0].exterior.coords)) \naoi = ee.FeatureCollection(pa_geom)\n\nWe also need to simplify the geometries in pa_tracts by reducing the number of vertices in each geometry using the specified tolerance of 0.01. The simplify method ensures that the topological integrity of the geometries is preserved. After simplification, the modified GeoDataFrame is converted to a Google Earth Engine (GEE) object using the geemap.geopandas_to_ee function, allowing for further geospatial analysis in the GEE platform, specifically when performing zonal statistics.\n\ntolerance = 0.01\npa_tracts['geometry'] = pa_tracts['geometry'].simplify(tolerance, preserve_topology=True)\npa_tracts_ee = geemap.geopandas_to_ee(pa_tracts)",
    "crumbs": [
      "Methodology",
      "Raster Data and Google Earth Engine"
    ]
  },
  {
    "objectID": "analysis/RasterData.html#loading-landsat-data",
    "href": "analysis/RasterData.html#loading-landsat-data",
    "title": "Raster Data and Google Earth Engine",
    "section": "Loading Landsat Data",
    "text": "Loading Landsat Data\nWe acquired all Landsat 8 level 2 images satellite images from Spring 2022 to Spring 2023 over the study area. These images were obtained via the Google Earth Engine API and are coded in the Earth Engine Data Catalog as USGS Landsat 8 Level 2, Collection 2, Tier 1. We acquired those images by season and took the average before further processing.\nThis specific version and level of processing of Landsat 8 satellite imagery provided by the United States Geological Survey uses Surface Reflectance Code to generate products with geometrical, radiometric, and atmospheric corrections. These products have a spatial resolution of 30 m. The products used in this study as predictors are the surface reflectance OLI bands, brightness temperature (BT), and some pre-processed indexes, such as the normalized difference vegetation index (NDVI), the soil-adjusted vegetation index (SAVI), and the enhanced vegetation index (EVI). Moreover, the images were processed to scale alll the bands and remove cloud coverage.\nSince we are taking into the account the impact of seasonal variabtions of vegetation indices on COPD, let’s first set up the constants that define the temporal extent of our study and transform them into a format that Earth Engine expects.\n\n## Define Time Period\nstartSpring = datetime(2022, 3, 1) # spring\nendSpring = datetime(2022, 5, 31)\nstartSummer = datetime(2022, 6, 1) # summer\nendSummer = datetime(2022, 8, 31)\nstartFall = datetime(2022, 9, 1) # fall\nendFall = datetime(2022, 11, 30)\nstartWinter = datetime(2022, 12, 1) # winter\nendWinter = datetime(2023, 2, 28)\n\n# Format dates into strings that Earth Engine expects (\"YYYY-MM-DD\")\nstartSpring= startSpring.strftime('%Y-%m-%d')\nendSpring = endSpring.strftime('%Y-%m-%d')\nstartSummer = startSummer.strftime('%Y-%m-%d')\nendSummer = endSummer.strftime('%Y-%m-%d')\nstartFall = startFall.strftime('%Y-%m-%d')\nendFall = endFall.strftime('%Y-%m-%d')\nstartWinter = startWinter.strftime('%Y-%m-%d')\nendWinter = endWinter.strftime('%Y-%m-%d')\n\nThe function apply_scale_factors applies scale and offset adjustments to Landsat satellite imagery bands, specifically for optical and thermal bands. In the Landsat data, pixel values are often represented as digital numbers (DN) which require conversion to physical units like reflectance or temperature.\n\n## Helper Function - Scale Bands\ndef apply_scale_factors(image):\n    # Scale and offset values for optical bands\n    optical_bands = image.select('SR_B.').multiply(0.0000275).add(-0.2)\n\n    # Scale and offset values for thermal bands\n    thermal_bands = image.select('ST_B.*').multiply(0.00341802).add(149.0)\n\n    # Add scaled bands to the original image\n    return image.addBands(optical_bands, None, True) \\\n                .addBands(thermal_bands, None, True)\n\nThe cloud_mask function is designed to create a binary mask for identifying and masking out pixels affected by clouds and cloud shadows in Landsat satellite imagery. It plays a crucial role in pre-processing Landsat imagery by removing cloud and cloud shadow effects to enhance data quality and reliability for downstream analysis.\n\n## Helper Function - Mask Clouds\ndef cloud_mask(image):\n    # Define cloud shadow and cloud bitmask (Bits 3 and 5)\n    cloud_shadow_bit_mask = 1 &lt;&lt; 3\n    cloud_bit_mask = 1 &lt;&lt; 5\n\n    # Select the Quality Assessment (QA) band for pixel quality information\n    qa = image.select('QA_PIXEL')\n\n    # Create a binary mask to identify clear conditions (both cloud and cloud shadow bits set to 0)\n    mask = qa.bitwiseAnd(cloud_shadow_bit_mask).eq(0) \\\n                .And(qa.bitwiseAnd(cloud_bit_mask).eq(0))\n\n    # Update the original image, masking out cloud and cloud shadow-affected pixels\n    return image.updateMask(mask)\n\nAs we breifly mentioned in the introduction, we acquired four different indices based on the bands of our satellite images. They are NDVI, SAVI, EVI, and LST. This function below will calculate all the indices we need all at once. Among those, NDVI is a common vegetation index used to assess the presence and health of vegetation based on the difference in reflectance between near-infrared (NIR) and red light wavelengths.\nThe formula is NDVI = (Band 5 – Band 4) / (Band 5 + Band 4)\nSAVI is a vegetation index similar to NDVI but incorporates a soil brightness correction factor to account for variations in soil reflectance.\nThe formulas is SAVI = ((Band 5 – Band 4) / (Band 5 + Band 4 + 0.5)) * (1.5)\nEVI is a vegetation index designed to minimize the influence of atmospheric conditions and background noise on vegetation assessments.\nThe formula is EVI = 2.5 * ((Band 5 – Band 4) / (Band 5 + 6 * Band 4 – 7.5 * Band 2 + 1))\nFinally, the calculation of land surface temperature (LST) needs to be broken down into the following steps:\n\nMinimum and Maximum NDVI Calculation: It calculates the minimum and maximum NDVI values within the AOI using the reduceRegion() method. The reducer parameter specifies the type of aggregation (in this case, min() and max())\nFraction of Vegetation (FV) Calculation: It computes the Fraction of Vegetation (FV) using the NDVI values, NDVI_min, and NDVI_max obtained in the previous step. The formula calculates the square of the normalized difference between NDVI and NDVI_min divided by the difference between NDVI_max and NDVI_min. `\nEmissivity (EM) Calculation: It calculates the emissivity using the FV values obtained from the previous step. The formula computes the emissivity based on the FV values according to the provided equation.\nLand Surface Temperature (LST) Calculation: It computes the Land Surface Temperature (LST) using the thermal band (Band 10) from the Landsat imagery and the emissivity values calculated earlier. The formula calculates the LST based on the Planck’s Law, considering the thermal band as temperature in Kelvin and the calculated emissivity.\n\n\ndef calculate_seasonal_indices(image_collection, aoi, season_name):\n    \n    # Calculate NDVI\n    ndvi = image_collection.normalizedDifference(['SR_B5', 'SR_B4']).rename('NDVI')\n\n    # Calculate SAVI\n    savi = image_collection.expression(\n        '1.5 * (NIR - RED) / (NIR + RED + 0.5)', {\n            'NIR': image_collection.select('SR_B5'),\n            'RED': image_collection.select('SR_B4')\n        }\n    ).rename('SAVI')\n\n    # Calculate EVI\n    evi = image_collection.expression(\n        '2.5 * ((NIR - RED) / (NIR + 6 * RED - 7.5 * BLUE + 1))', {\n            'NIR': image_collection.select('SR_B5'),\n            'RED': image_collection.select('SR_B4'),\n            'BLUE': image_collection.select('SR_B2')\n        }\n    ).rename('EVI')\n\n    # NDVI min and max for Fraction of Vegetation (FV) calculation\n    ndvi_min = ndvi.reduceRegion(\n        reducer=ee.Reducer.min(),\n        geometry=aoi,\n        scale=30,\n        maxPixels=1e9\n    ).get('NDVI')\n\n    ndvi_max = ndvi.reduceRegion(\n        reducer=ee.Reducer.max(),\n        geometry=aoi,\n        scale=30,\n        maxPixels=1e9\n    ).get('NDVI')\n\n    # Convert NDVI_min and NDVI_max to ee.Number\n    ndvi_min = ee.Number(ndvi_min)\n    ndvi_max = ee.Number(ndvi_max)\n\n    # Fraction of Vegetation (FV)\n    fv = ndvi.subtract(ndvi_min).divide(ndvi_max.subtract(ndvi_min)).pow(2).rename('FV')\n\n    # Emissivity (EM)\n    em = fv.multiply(0.004).add(0.986).rename('EM')\n\n    # Thermal band (Band 10)\n    thermal = image_collection.select('ST_B10').rename('thermal')\n\n    # Land Surface Temperature (LST)\n    lst = thermal.expression(\n        '(TB / (1 + (0.00115 * (TB / 1.438)) * log(em))) - 273.15',\n        {\n            'TB': thermal.select('thermal'),  # Thermal band temperature in Kelvin\n            'em': em  # Emissivity\n        }\n    ).rename('LST')\n\n    seasonal_image = ndvi.addBands([savi, evi, fv, em, lst])\n    return seasonal_image\n\nWe define a dictionary that maps each season to its corresponding start and end dates. The code then iterates through each season, filters a Landsat 8 image collection based on the region of interest (aoi) and the defined seasonal date range, applies scale factors and a cloud mask, computes the median composite of the images, and clips the result to the aoi. For each season, the processed image collection is passed to the calculate_seasonal_indices function, which calculates season-specific indices (e.g., NDVI, SAVI).\n\nseasons = {\n    'spring': (startSpring, endSpring),\n    'summer': (startSummer, endSummer),\n    'fall': (startFall, endFall),\n    'winter': (startWinter, endWinter)\n}\n\nseasonal_results = {}\nfor season, (start_date, end_date) in seasons.items():\n    image_collection = ee.ImageCollection(\"LANDSAT/LC08/C02/T1_L2\") \\\n        .filterBounds(aoi) \\\n        .filterDate(start_date, end_date) \\\n        .map(apply_scale_factors) \\\n        .map(cloud_mask) \\\n        .median() \\\n        .clip(aoi)\n\n    seasonal_results[season] = calculate_seasonal_indices(image_collection, aoi, season)\n\nNext, the function below helps to summarize our calculated indicies into census tracts using zonal statistics. This is one of the easiest method of brining raster information into vector geometries. In Earth Engine, zonal statistics can be calculated by aggregating pixel values within geometries defined by feature collections (in our case, the census tracts). Common summary statistics computed for each zone include mean, sum, minimum, maximum, standard deviation, and percentile values (in our case, we will calculate the mean). We wrote the export zonal statistics function and by default, it will write the output directly to google drive.\n\n# Function to export zonal stats to Google Drive\ndef export_zonal_stats(image, reducer, file_name, folder_name=\"######\"):\n    \n    zonal_stats = image.reduceRegions(\n        collection=pa_tracts_ee,\n        reducer = ee.Reducer.mean()\n        scale=30  # Resolution of the analysis\n    )\n\n    task = ee.batch.Export.table.toDrive(\n        collection=zonal_stats,\n        fileFormat='CSV',\n        fileNamePrefix=file_name.replace('.csv', ''),\n        folder=folder_name\n    )\n    task.start()\n    print(f\"Export started for {file_name}. Check Google Drive for the results.\")\n\nThe code below will give us 16 separate csv files, each with four columns. The system:index is a unique identifier for each census tract, the mean is the mean pixels values, the .geo is the geometry of each census tracts, and GEOID is the identifier for each census tract.\n\nseasonal_results = {\n    \"spring\": seasonal_results['spring'],\n    \"summer\": seasonal_results['summer'],\n    \"fall\": seasonal_results['fall'],\n    \"winter\": seasonal_results['winter']\n}\n\nbands = ['NDVI', 'EVI', 'SAVI', 'LST']\n\nfor season, image in seasonal_results.items():\n    for band in bands:\n        band_image = image.select(band)  # Extract specific band\n        file_name = f\"{band.lower()}_{season}.csv\"  # File name e.g., ndvi_spring.csv\n        export_zonal_stats(image=band_image, reducer=reducer, file_name=file_name)",
    "crumbs": [
      "Methodology",
      "Raster Data and Google Earth Engine"
    ]
  },
  {
    "objectID": "analysis/RasterData.html#loading-land-cover-data",
    "href": "analysis/RasterData.html#loading-land-cover-data",
    "title": "Raster Data and Google Earth Engine",
    "section": "Loading Land Cover Data",
    "text": "Loading Land Cover Data\nWe also acquired USGS National Land Cover Data for our study area. These images were also obtained via the Google Earth Engine API and are coded in the Earth Engine Data Catalog. We use the 2021 data because that is the latest release avaialbe on Earth Engine and we assume that the landcover of 2021 would match with that of 2022. Afterloading the data, filter is applied to extract the land cover data for the year 2021. The select method isolates the landcover band, which contains the land cover classification. Then, the data is clipped to the specified area of interest (aoi), creating a subset focused on Pennsylvania.\n\ndataset = ee.ImageCollection('USGS/NLCD_RELEASES/2021_REL/NLCD')\nnlcd2021 = dataset.filter(ee.Filter.eq('system:index', '2021')).first()\nlandcover = nlcd2021.select('landcover')\npa_landcover = landcover.clip(aoi)\n\nThis landcover dataset is publicly available and consists of 8 main land cover classes and 20 sub-classes of land cover. These classes include Water (open water and perennial ice/snow), Developed (high, medium, low and open space), Barren (rock/sand/clay), Forest (deciduous, evergreen and mixed), Shrubland (dwarf shrub and shrub), Herbaceous (grassland, sedge, lichens and moss), Planted/ Cultivated (pasture/hay and cultivated crops) and Wetlands (woody and emergent herbaceous). For detailed land cover classification, refer to the chart below:\n\nIn this study, various land cover types are grouped into broader categories based on their respective NLCD codes for simplicty of analysis. For example, high-density areas are defined as land cover types 23 and 24, which typically represent urban environments. Similarly, low-density areas (codes 21 and 22) correspond to open spaces like barren land and low-density residential areas. Forested areas are grouped under codes 41, 42, and 43, while grasses are categorized with codes 52, 71, 81, and 82. Wetlands and open water are also identified using their specific codes (90, 95, and 11). This categorization simplifies the analysis by grouping similar land cover types together based on their characteristics, enabling focused analysis of specific land use patterns.\n\nhigh_density = pa_landcover.eq(23).Or(pa_landcover.eq(24))\nlow_density = pa_landcover.eq(21).Or(pa_landcover.eq(22))\nforest = pa_landcover.eq(41).Or(pa_landcover.eq(42)).Or(pa_landcover.eq(43))\ngrasses = pa_landcover.eq(52).Or(pa_landcover.eq(71)).Or(pa_landcover.eq(81)).Or(pa_landcover.eq(82))\nwetlands = pa_landcover.eq(90).Or(pa_landcover.eq(95))\nopen_water = pa_landcover.eq(11)\n\nAs briefly mentioned in the workflow, given that these proportions can be similar across multiple census tracts despite the underlying distribution of land cover being different (e.g., two census tracts can have the same percentage of highly developed land, but that land can be geographically concentrated in one tract and distributed in the other), we introduced additional features to represent the distribution of land cover features. For each census tract we calculate features representing i) the proportion of high developed land pixels neighboring another high developed land pixel and ii) the proportion of forest pixels neighboring another forest pixel. These features were intended to capture the notion that developed land corresponds to higher level of pollution while forests and trees can be a source of pollen, both of which have adverse impact on patients suffering from asthma and COPD.\nThe code below achieves this goal. It first creates a kernel for identifying neighboring pixels, then calculates the total number of land cover pixels within each region. It also computes the number of neighboring land cover pixels using a convolution operation to identify adjacent pixels. These calculations are performed using the reduceRegions method to aggregate the data for each region of interest (e.g., census tracts) from the input land cover mask. The results are then merged, associating the total land cover and neighboring land cover values with their corresponding region identifiers (GEOID). Finally, the function exports the aggregated data as a CSV file to Google Drive.\n\ndef neighboring_landcover_metrics(landcover_mask, file_name):\n\n    # Define the kernel for neighboring pixels\n    kernel = ee.Kernel.square(radius=1, units='pixels')  # 3x3 neighborhood\n    neighbors = landcover_mask.convolve(kernel).gte(1)  # At least one neighbor\n\n    # Calculate total landcover pixels\n    total_landcover = landcover_mask.reduceRegions(\n        collection=pa_tracts_ee,\n        reducer=ee.Reducer.sum(),\n        scale=30\n    ).select(['sum'], ['total_landcover'])\n\n    # Calculate neighboring landcover pixels\n    neighbor_landcover = neighbors.reduceRegions(\n        collection=pa_tracts_ee,\n        reducer=ee.Reducer.sum(),\n        scale=30\n    ).select(['sum'], ['neighbor_landcover'])\n\n    # Merge FeatureCollections and retain geoid\n    merged_fc = total_landcover.map(lambda feature:\n        feature.set(\n            'neighbor_landcover',\n            neighbor_landcover.filter(ee.Filter.eq('system:index', feature.get('system:index')))\n                              .first()\n                              .get('neighbor_landcover')\n        ).set(\n            'geoid', pa_tracts_ee.filter(ee.Filter.eq('system:index', feature.get('system:index')))\n                                 .first()\n                                 .get('GEOID')\n        )\n    )\n\n    # Export the merged FeatureCollection\n    export_task = ee.batch.Export.table.toDrive(\n        collection=merged_fc.select(['geoid', 'total_landcover', 'neighbor_landcover']),\n        folder='#######',\n        fileNamePrefix=file_name,\n        fileFormat='CSV'\n    )\n    export_task.start()\n    print(f\"Export task started: {file_name}\")\n\nFor forest and high-density land cover types, the neighboring_landcover_metrics function is applied to calculate the number of neighboring land cover pixels within a 3x3 pixel neighborhood.\n\nneighboring_landcover_metrics(\n    landcover_mask=forest,\n    file_name='forest_landcover_metrics'\n)\n\nneighboring_landcover_metrics(\n    landcover_mask=high_density,\n    file_name='high_density_landcover_metrics'\n)\n\nFor other land cover types such as grasses, low density, wetlands, and open water, the summarize_landcover_pixels function is used instead. This function simply calculates the total number of land cover pixels within each region, without considering neighboring pixels. The results for both types of metrics are then exported as CSV files to Google Drive for further analysis.\n\ndef summarize_landcover_pixels(landcover_mask, file_name):\n    \n    # Calculate total landcover pixels\n    total_landcover = landcover_mask.reduceRegions(\n        collection=pa_tracts_ee,\n        reducer=ee.Reducer.sum(),\n        scale=30\n    ).map(lambda feature: feature.set(\n        'geoid', feature.get('GEOID')\n    ))\n\n    # Export the results to Drive\n    export_task = ee.batch.Export.table.toDrive(\n        collection=total_landcover.select(['geoid', 'sum']),\n        folder='#######',\n        fileNamePrefix=file_name,\n        fileFormat='CSV'\n    )\n    export_task.start()\n    print(f\"Export task started: {file_name}\")\n\n\nlandcover_list = [\n    {'mask': grasses, 'file_name': 'grasses_landcover'},\n    {'mask': low_density, 'file_name': 'low_density_landcover'},\n    {'mask': wetlands, 'file_name': 'wetlands_landcover'},\n    {'mask': open_water, 'file_name': 'open_water_landcover'}\n]\n\nfor landcover in landcover_list:\n    summarize_landcover_pixels(landcover['mask'], landcover['file_name'])",
    "crumbs": [
      "Methodology",
      "Raster Data and Google Earth Engine"
    ]
  },
  {
    "objectID": "code/Data_Analysis.html",
    "href": "code/Data_Analysis.html",
    "title": "Further Data Analysis Script",
    "section": "",
    "text": "from google.colab import drive\ndrive.mount('/content/drive')\n\nMounted at /content/drive\n\n\n\nimport os\nimport pandas as pd\nimport numpy as np\nimport geopandas as gpd\nfrom shapely.geometry import shape, Polygon, Point\nfrom libpysal.weights import Queen\nfrom libpysal.weights.spatial_lag import lag_spatial\n\n\n# read tracts data\nPA_Tracts = gpd.read_file('/content/drive/MyDrive/data/PA_Tracts.geojson')\n\n\n# read cdc data\nPA_Chronic = pd.read_csv('/content/drive/MyDrive/data/PA_Chronic.csv')\nPA_HRB = pd.read_csv('/content/drive/MyDrive/data/PA_HRB.csv')\n\n\n# process cdc data\nPA_Chronic = PA_Chronic[[\"CountyName\", \"LocationName\", \"Asthma\", \"TotalPopulation\", \"TotalPop18plus\", \"COP\"]]\nPA_HRB = PA_HRB[[\"CountyName\", \"LocationName\", \"Smoking\", \"Drinking\", \"Short_Sleep\", \"Physical_Activity\"]]\n\n\n# read tobacco data\nPA_Retailers = pd.read_csv('/content/drive/MyDrive/data/PA_Retailers.csv')\n\n\n# process retailers data\nPA_Retailers['geometry'] = PA_Retailers.apply(\n    lambda row: Point(row['lon'], row['lat']), axis=1\n)\n\nPA_Retailers = gpd.GeoDataFrame(\n    PA_Retailers,\n    geometry='geometry',\n    crs='EPSG:4326'\n)\n\n\n# count retailers\ntracts_with_retailers = gpd.sjoin(PA_Tracts, PA_Retailers, how=\"left\", predicate=\"intersects\")\ntracts_summary = (\n    tracts_with_retailers.groupby(\"GEOID\")\n    .size()\n    .reset_index(name=\"total_retailers\")\n)\n\ntracts_with_retailers= PA_Tracts.merge(tracts_summary, on=\"GEOID\", how=\"left\")\ntracts_with_retailers[\"total_retailers\"] = tracts_with_retailers[\"total_retailers\"].fillna(0)\n\n# compute density\ndensity = tracts_with_retailers.to_crs(epsg=3857)\ndensity['area_km2'] = density.geometry.area / 1e6  # Convert m² to km²\ndensity ['density'] = density ['total_retailers'] / density ['area_km2']\n\n\n  \n    \n\n\n\n\n\n\nGEOID\ngeometry\ntotal_retailers\narea_km2\ndensity\n\n\n\n\n0\n42001030101\nPOLYGON ((-8588920.357 4874186.492, -8588818.5...\n3\n95.080052\n0.031552\n\n\n1\n42001030103\nPOLYGON ((-8579678.266 4862280.523, -8579641.3...\n1\n31.775960\n0.031470\n\n\n2\n42001030104\nPOLYGON ((-8582635.361 4858365.994, -8582454.6...\n7\n86.167356\n0.081237\n\n\n3\n42001030200\nPOLYGON ((-8599802.938 4869004.417, -8599007.8...\n8\n207.033802\n0.038641\n\n\n4\n42001030300\nPOLYGON ((-8618506.453 4863224.423, -8618215.5...\n5\n191.098396\n0.026165\n\n\n...\n...\n...\n...\n...\n...\n\n\n3441\n42133023902\nPOLYGON ((-8526772.659 4837173.636, -8526669.9...\n1\n98.319324\n0.010171\n\n\n3442\n42133023903\nPOLYGON ((-8532719.338 4831745.365, -8532693.1...\n8\n31.425125\n0.254573\n\n\n3443\n42133023904\nPOLYGON ((-8535019.091 4834550.597, -8534977.0...\n4\n89.787440\n0.044550\n\n\n3444\n42133024001\nPOLYGON ((-8506177.304 4828656.467, -8506121.8...\n11\n131.697087\n0.083525\n\n\n3445\n42133024002\nPOLYGON ((-8519040.515 4827590.268, -8519035.7...\n5\n126.176784\n0.039627\n\n\n\n\n3446 rows × 5 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n\n# Create spatial weights matrix based on tract geometries\nweights = Queen.from_dataframe(density)\n\n# Compute average density of neighbors using lag_spatial\ndensity['neighbor_avg_density'] = lag_spatial(\n    weights, density['density']\n)\n\nFutureWarning: `use_index` defaults to False but will default to True in future. Set True/False directly to control this behavior and silence this warning\n  weights = Queen.from_dataframe(density)\n\n\n\nPA_Retailers = density.drop(columns=[\"area_km2\"])\nPA_Retailers.head(5)\n\n\n  \n    \n\n\n\n\n\n\nGEOID\ngeometry\ntotal_retailers\ndensity\nneighbor_avg_density\n\n\n\n\n0\n42001030101\nPOLYGON ((-8588920.357 4874186.492, -8588818.5...\n3\n0.031552\n0.227339\n\n\n1\n42001030103\nPOLYGON ((-8579678.266 4862280.523, -8579641.3...\n1\n0.031470\n0.158547\n\n\n2\n42001030104\nPOLYGON ((-8582635.361 4858365.994, -8582454.6...\n7\n0.081237\n1.671000\n\n\n3\n42001030200\nPOLYGON ((-8599802.938 4869004.417, -8599007.8...\n8\n0.038641\n0.413897\n\n\n4\n42001030300\nPOLYGON ((-8618506.453 4863224.423, -8618215.5...\n5\n0.026165\n0.216389\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\n# read census data\nPA_Census = pd.read_csv('/content/drive/MyDrive/data/PA_Census_Data.csv')\n\n\nPA_Census.head(5)\n\n\n  \n    \n\n\n\n\n\n\nGEOID\nminority\naging\ndisability\ngeometry\n\n\n\n\n0\n42125775200\n0.211840\n0.215264\n0.287671\nPOLYGON ((-79.876586 40.177549, -79.876234 40....\n\n\n1\n42125775300\n0.395709\n0.131506\n0.273739\nPOLYGON ((-79.879294 40.174857, -79.878454 40....\n\n\n2\n42125782700\n0.048652\n0.314267\n0.302433\nPOLYGON ((-79.913564 40.153257, -79.913332 40....\n\n\n3\n42125783200\n0.167610\n0.160644\n0.229430\nPOLYGON ((-79.911421 40.144556, -79.909319 40....\n\n\n4\n42125783300\n0.167792\n0.166234\n0.159481\nPOLYGON ((-79.906306 40.137474, -79.906219 40....\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\n# read land cover data\nfolder_path = '/content/drive/MyDrive/data/PA_Landcover/'\n\ncsv_files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]\n\nlandcover = []\nfor file in csv_files:\n    file_path = os.path.join(folder_path, file)\n    first_word = file.split()[0]\n    df = pd.read_csv(file_path)\n    df['source_file'] = first_word\n    df = df.drop(columns=[\"system:index\", \".geo\"])\n    landcover.append(df)\n\n\n# combine landcover data\nmerged_df= pd.merge(landcover[0], landcover[1], on=\"geoid\", how=\"left\")\nmerged_df = merged_df.rename(\n    columns={\n        \"total_landcover\": \"forest_total\",\n        \"neighbor_landcover\": \"forest_neighbor\",\n        \"sum\": \"wetland_total\",\n    }\n)\nmerged_df = merged_df.drop(columns=[\"source_file_x\", \"source_file_y\"])\nmerged_df = pd.merge(merged_df, landcover[2], on=\"geoid\", how=\"left\")\nmerged_df = merged_df.rename(\n    columns={\n        \"total_landcover\": \"Hdensity_total\",\n        \"neighbor_landcover\": \"Hdensity_neighbor\"    }\n)\nmerged_df = merged_df.drop(columns=[\"source_file\"])\nmerged_df = pd.merge(merged_df, landcover[3], on=\"geoid\", how=\"left\")\nmerged_df = merged_df.rename(\n    columns={\n        \"sum\": \"Ldensity_total\"    }\n)\nmerged_df = merged_df.drop(columns=[\"source_file\"])\nmerged_df = pd.merge(merged_df, landcover[4], on=\"geoid\", how=\"left\")\nmerged_df = merged_df.rename(\n    columns={\n        \"sum\": \"grasses_total\"    }\n)\nmerged_df = merged_df.drop(columns=[\"source_file\"])\nmerged_df = pd.merge(merged_df, landcover[5], on=\"geoid\", how=\"left\")\nmerged_df = merged_df.rename(\n    columns={\n        \"sum\": \"water_total\"    }\n)\nPA_Landcover = merged_df.drop(columns=[\"source_file\"])\n\n\n# some metrics with landcover data\nPA_Landcover['forest_pct'] = PA_Landcover['forest_neighbor'] / PA_Landcover['forest_total']\nPA_Landcover['Hdensity_pct'] = PA_Landcover['Hdensity_neighbor'] / PA_Landcover['Hdensity_total']\nPA_Landcover = PA_Landcover.drop(columns=[\"forest_neighbor\", \"Hdensity_neighbor\"])\n\n\nPA_Landcover.head(5)\n\n\n  \n    \n\n\n\n\n\n\ngeoid\nforest_total\nwetland_total\nHdensity_total\nLdensity_total\ngrasses_total\nwater_total\nforest_pct\nHdensity_pct\n\n\n\n\n0\n42001030101\n14626.749020\n2675.717647\n817.870588\n5868.972549\n31291.925490\n288.745098\n0.470716\n0.084107\n\n\n1\n42001030103\n5035.964706\n408.607843\n283.894118\n3644.113725\n11254.184314\n879.866667\n0.407744\n0.042269\n\n\n2\n42001030104\n10099.086275\n2142.858824\n1041.843137\n6041.019608\n40245.227451\n405.870588\n0.354698\n0.032345\n\n\n3\n42001030200\n34923.054902\n5905.745098\n2194.835294\n13370.447059\n80858.125490\n214.356863\n0.429251\n0.127547\n\n\n4\n42001030300\n57866.054902\n1336.996078\n1100.000000\n9721.752941\n40909.298039\n348.709804\n0.718114\n0.203636\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\n# read in landsat data\nfolder_path = '/content/drive/MyDrive/data/PA_Landsat/'\n\ncsv_files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]\n\nlandsat = []\nfor file in csv_files:\n    file_path = os.path.join(folder_path, file)\n    first_word = file.split()[0]\n    df = pd.read_csv(file_path)\n    df['source_file'] = first_word\n    df = df.drop(columns=[\"system:index\", \".geo\"])\n    landsat.append(df)\n\n\nmerged_df2= pd.merge(landsat[0], landsat[1], on=\"GEOID\", how=\"left\")\nmerged_df2 = merged_df2.rename(\n    columns={\n        \"mean_x\": \"lst_winter\",\n        \"mean_y\": \"ndvi_fall\",\n    }\n)\nmerged_df2 = merged_df2.drop(columns=[\"source_file_x\", \"source_file_y\"])\nmerged_df2 = pd.merge(merged_df2, landsat[2], on=\"GEOID\", how=\"left\")\nmerged_df2 = merged_df2.rename(\n    columns={\n        \"mean\": \"evi_spring\",\n    }\n)\nmerged_df2 = merged_df2.drop(columns=[\"source_file\"])\nmerged_df2 = pd.merge(merged_df2, landsat[3], on=\"GEOID\", how=\"left\")\nmerged_df2 = merged_df2.rename(\n    columns={\n        \"mean\": \"evi_fall\",\n    }\n)\nmerged_df2 = merged_df2.drop(columns=[\"source_file\"])\nmerged_df2 = pd.merge(merged_df2, landsat[4], on=\"GEOID\", how=\"left\")\nmerged_df2 = merged_df2.rename(\n    columns={\n        \"mean\": \"savi_fall\",\n    }\n)\nmerged_df2 = merged_df2.drop(columns=[\"source_file\"])\nmerged_df2 = pd.merge(merged_df2, landsat[5], on=\"GEOID\", how=\"left\")\nmerged_df2 = merged_df2.rename(\n    columns={\n        \"mean\": \"savi_winter\",\n    }\n)\nmerged_df2 = merged_df2.drop(columns=[\"source_file\"])\nmerged_df2 = pd.merge(merged_df2, landsat[6], on=\"GEOID\", how=\"left\")\nmerged_df2 = merged_df2.rename(\n    columns={\n        \"mean\": \"savi_summer\",\n    }\n)\nmerged_df2 = merged_df2.drop(columns=[\"source_file\"])\nmerged_df2 = pd.merge(merged_df2, landsat[7], on=\"GEOID\", how=\"left\")\nmerged_df2 = merged_df2.rename(\n    columns={\n        \"mean\": \"savi_spring\",\n    }\n)\nmerged_df2 = merged_df2.drop(columns=[\"source_file\"])\nmerged_df2 = pd.merge(merged_df2, landsat[8], on=\"GEOID\", how=\"left\")\nmerged_df2 = merged_df2.rename(\n    columns={\n        \"mean\": \"ndvi_winter\",\n    }\n)\nmerged_df2 = merged_df2.drop(columns=[\"source_file\"])\nmerged_df2 = pd.merge(merged_df2, landsat[9], on=\"GEOID\", how=\"left\")\nmerged_df2 = merged_df2.rename(\n    columns={\n        \"mean\": \"lst_summer\",\n    }\n)\nmerged_df2 = merged_df2.drop(columns=[\"source_file\"])\nmerged_df2 = pd.merge(merged_df2, landsat[10], on=\"GEOID\", how=\"left\")\nmerged_df2 = merged_df2.rename(\n    columns={\n        \"mean\": \"evi_summer\",\n    }\n)\nmerged_df2 = merged_df2.drop(columns=[\"source_file\"])\nmerged_df2 = pd.merge(merged_df2, landsat[11], on=\"GEOID\", how=\"left\")\nmerged_df2 = merged_df2.rename(\n    columns={\n        \"mean\": \"lst_spring\",\n    }\n)\nmerged_df2 = merged_df2.drop(columns=[\"source_file\"])\nmerged_df2 = pd.merge(merged_df2, landsat[12], on=\"GEOID\", how=\"left\")\nmerged_df2 = merged_df2.rename(\n    columns={\n        \"mean\": \"lst_fall\",\n    }\n)\nmerged_df2 = merged_df2.drop(columns=[\"source_file\"])\nmerged_df2 = pd.merge(merged_df2, landsat[13], on=\"GEOID\", how=\"left\")\nmerged_df2 = merged_df2.rename(\n    columns={\n        \"mean\": \"ndvi_spring\",\n    }\n)\nmerged_df2 = merged_df2.drop(columns=[\"source_file\"])\nmerged_df2 = pd.merge(merged_df2, landsat[14], on=\"GEOID\", how=\"left\")\nmerged_df2 = merged_df2.rename(\n    columns={\n        \"mean\": \"ndvi_summer\",\n    }\n)\nmerged_df2 = merged_df2.drop(columns=[\"source_file\"])\nmerged_df2 = pd.merge(merged_df2, landsat[15], on=\"GEOID\", how=\"left\")\nmerged_df2 = merged_df2.rename(\n    columns={\n        \"mean\": \"evi_winter\",\n    }\n)\nPA_Landsat = merged_df2.drop(columns=[\"source_file\"])\n\n\nPA_Landsat.head()\n\n\n  \n    \n\n\n\n\n\n\nGEOID\nlst_winter\nndvi_fall\nevi_spring\nevi_fall\nsavi_fall\nsavi_winter\nsavi_summer\nsavi_spring\nndvi_winter\nlst_summer\nevi_summer\nlst_spring\nlst_fall\nndvi_spring\nndvi_summer\nevi_winter\n\n\n\n\n0\n42001030101\n4.714395\n0.672438\n0.319802\n0.410165\n0.395006\n0.263375\n0.517445\n0.316083\n0.473559\n33.257296\n0.568514\n10.827598\n19.379386\n0.501817\n0.762120\n0.260378\n\n\n1\n42001030103\n4.470871\n0.620192\n0.291822\n0.349453\n0.341787\n0.241329\n0.491634\n0.288043\n0.434709\n33.251842\n0.537638\n9.268735\n19.461976\n0.470976\n0.736301\n0.238932\n\n\n2\n42001030104\n4.283530\n0.616475\n0.337012\n0.368580\n0.359570\n0.279969\n0.512783\n0.328228\n0.468789\n33.716166\n0.563929\n10.559965\n19.039489\n0.509266\n0.753435\n0.280906\n\n\n3\n42001030200\n3.399675\n0.664453\n0.351469\n0.406248\n0.392448\n0.260190\n0.522249\n0.343635\n0.459103\n33.166265\n0.575393\n9.763646\n18.368969\n0.526715\n0.764207\n0.256791\n\n\n4\n42001030300\n2.016297\n0.706754\n0.317563\n0.415473\n0.408640\n0.249940\n0.566242\n0.317692\n0.500742\n30.361017\n0.629287\n10.687916\n16.265277\n0.535982\n0.821622\n0.239779\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\n# everthing combined\nPA_Retailers['GEOID'] = PA_Retailers['GEOID'].astype(str)\nPA_Landsat['GEOID'] = PA_Landsat['GEOID'].astype(str)\nPA_Landcover['GEOID'] = PA_Landcover['geoid'].astype(str)\nPA_Census['GEOID'] = PA_Census['GEOID'].astype(str)\nPA_Chronic['GEOID'] = PA_Chronic['LocationName'].astype(str)\nPA_HRB['GEOID'] = PA_HRB['LocationName'].astype(str)\n\nPA_Final = pd.merge(PA_Chronic, PA_HRB[[\"GEOID\", \"Smoking\", \"Drinking\", \"Short_Sleep\", \"Physical_Activity\"]], on=\"GEOID\", how=\"inner\")\nPA_Final = pd.merge(PA_Final, PA_Retailers[[\"GEOID\", \"neighbor_avg_density\"]],  on=\"GEOID\", how=\"inner\")\nPA_Final = pd.merge(PA_Final, PA_Census[[\"GEOID\", \"minority\", \"aging\", \"disability\"]],  on=\"GEOID\", how=\"inner\")\nPA_Final = pd.merge(PA_Final, PA_Landcover.drop(columns=[\"geoid\"]),  on=\"GEOID\", how=\"inner\")\nPA_Final = pd.merge(PA_Final, PA_Landsat,  on=\"GEOID\", how=\"inner\")\nPA_Final = PA_Final.fillna(0)\n\n\nPA_Final.head(5)\n\n\n  \n    \n\n\n\n\n\n\nCountyName\nLocationName\nAsthma\nTotalPopulation\nTotalPop18plus\nCOP\nGEOID\nSmoking\nDrinking\nShort_Sleep\n...\nsavi_summer\nsavi_spring\nndvi_winter\nlst_summer\nevi_summer\nlst_spring\nlst_fall\nndvi_spring\nndvi_summer\nevi_winter\n\n\n\n\n0\nAllegheny\n42003141200\n9.7\n4007\n3242\n4.8\n42003141200\n10.3\n21.3\n32.1\n...\n0.342207\n0.274674\n0.316698\n38.621956\n0.359755\n27.566184\n23.110598\n0.469827\n0.569421\n0.118931\n\n\n1\nAllegheny\n42003140100\n10.6\n5579\n5066\n4.2\n42003140100\n10.7\n23.9\n34.5\n...\n0.365760\n0.297535\n0.347601\n37.710135\n0.397444\n27.806910\n21.461009\n0.488617\n0.570255\n0.155803\n\n\n2\nAllegheny\n42003191900\n10.6\n2177\n1786\n5.9\n42003191900\n14.6\n22.1\n35.0\n...\n0.307345\n0.272597\n0.277480\n36.410736\n0.324332\n32.850463\n21.101579\n0.446655\n0.489839\n0.135087\n\n\n3\nAllegheny\n42003320700\n10.7\n1613\n1318\n7.3\n42003320700\n16.4\n21.3\n36.0\n...\n0.430196\n0.350588\n0.367408\n36.050876\n0.472156\n28.288317\n19.085399\n0.565722\n0.654718\n0.135720\n\n\n4\nAllegheny\n42003100500\n11.3\n2013\n1695\n7.7\n42003100500\n15.3\n15.8\n40.6\n...\n0.414667\n0.330390\n0.356294\n36.341476\n0.456215\n30.983730\n20.809972\n0.538620\n0.632465\n0.161533\n\n\n\n\n5 rows × 39 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\n# join with geometry if needed\nPA_Final_Geom = PA_Final.merge(PA_Tracts[[\"GEOID\", \"geometry\"]], on=\"GEOID\", how=\"left\")\nPA_Final_Geom = gpd.GeoDataFrame(PA_Final_Geom, geometry='geometry')\n\n\nPA_Final.to_csv('/content/drive/MyDrive/PA_Final.csv', index=False)\nPA_Final_Geom.to_file('/content/drive/MyDrive/PA_Final_Geom.geojson', driver='GeoJSON')\n\n\n\n\n Back to top"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Spatial Estimation of Seasonal Variability in Chronic Obtrusive Pulmonary Disease Prevalence Based on Geospatial Machine Learning Procedures",
    "section": "",
    "text": "Estimating disease prevalence at a sub-city neighborhood scale enables early, targeted interventions that can save lives and reduce public health burdens. However, accurately identifying neighborhood-level disease prevalence is challenging due to the cost-prohibitive nature of localized data collection, difficulties in accessing confidential medical records, and the complexity of interactions between multiple potential determinants.\nTo address these challenges and capture a multidimensional perspective on the determinants of chronic respiratory diseases, this study investigates the environmental, socio-demographic, and behavioral predictors of chronic obstructive pulmonary disease (COPD) at the census tract level in Pennsylvania using machine learning models. By localizing the analysis to this geographic granularity, we aim to gain a nuanced understanding of spatial disparities in respiratory health. Drawing on data from tobacco retailer density, census records, health risk behavior surveys, seasonally classified land cover datasets, and satellite-derived vegetation indices, the study identifies the most significant predictors of COPD prevalence. It also evaluates the performance of various analytical techniques, including multilayer perceptron, support vector regression, and random forest regression.\nPreliminary results suggest that local environmental factors have a slightly stronger influence on COPD prevalence during the spring and fall, with development density and tobacco retailer coverage emerging as prominent predictors. The random forest model demonstrated strong predictive performance. Initial findings reveal that COPD prevalence is higher in several urban and very rural tracts while lower in suburban areas. Ultimately, this project aims to enhance the prediction of chronic respiratory diseases and provide valuable insights into the interplay of environmental and socioeconomic factors in shaping respiratory health outcomes across Pennsylvania.\nGithub Repository: MUSA550-PA-CRD-Prediction\n\nContributor:\nEmily Zhou\nDepartment of City and Regional Planning, Stuart Weitzman School of Design, University of Pennsylvania\nKey Words:\nchronic respiratory disease, support vector machine, random forest, multiple layer perceptron, deep learning, bayesian information criteria, google earth engine, land cover, geospatial health",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#abstract",
    "href": "index.html#abstract",
    "title": "Spatial Estimation of Seasonal Variability in Chronic Obtrusive Pulmonary Disease Prevalence Based on Geospatial Machine Learning Procedures",
    "section": "",
    "text": "Estimating disease prevalence at a sub-city neighborhood scale enables early, targeted interventions that can save lives and reduce public health burdens. However, accurately identifying neighborhood-level disease prevalence is challenging due to the cost-prohibitive nature of localized data collection, difficulties in accessing confidential medical records, and the complexity of interactions between multiple potential determinants.\nTo address these challenges and capture a multidimensional perspective on the determinants of chronic respiratory diseases, this study investigates the environmental, socio-demographic, and behavioral predictors of chronic obstructive pulmonary disease (COPD) at the census tract level in Pennsylvania using machine learning models. By localizing the analysis to this geographic granularity, we aim to gain a nuanced understanding of spatial disparities in respiratory health. Drawing on data from tobacco retailer density, census records, health risk behavior surveys, seasonally classified land cover datasets, and satellite-derived vegetation indices, the study identifies the most significant predictors of COPD prevalence. It also evaluates the performance of various analytical techniques, including multilayer perceptron, support vector regression, and random forest regression.\nPreliminary results suggest that local environmental factors have a slightly stronger influence on COPD prevalence during the spring and fall, with development density and tobacco retailer coverage emerging as prominent predictors. The random forest model demonstrated strong predictive performance. Initial findings reveal that COPD prevalence is higher in several urban and very rural tracts while lower in suburban areas. Ultimately, this project aims to enhance the prediction of chronic respiratory diseases and provide valuable insights into the interplay of environmental and socioeconomic factors in shaping respiratory health outcomes across Pennsylvania.\nGithub Repository: MUSA550-PA-CRD-Prediction\n\nContributor:\nEmily Zhou\nDepartment of City and Regional Planning, Stuart Weitzman School of Design, University of Pennsylvania\nKey Words:\nchronic respiratory disease, support vector machine, random forest, multiple layer perceptron, deep learning, bayesian information criteria, google earth engine, land cover, geospatial health",
    "crumbs": [
      "Home"
    ]
  }
]